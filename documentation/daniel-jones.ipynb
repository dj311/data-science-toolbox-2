{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection on the KDD Cup 99 Data Set: Documentation\n",
    "*— Daniel Jones*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries for Python session (see `readme.md` for installation instructions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib  # graphs and plotting for python\n",
    "import numpy  # fast arrays for python (used by pandas)\n",
    "import pandas  # provides dataframe's and similar structures for python\n",
    "import seaborn  # provides pre-configured, pretty graphs for matplotlib\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Jupyter with `rpy2` to allow embedding R, and `matplotlib` to allow inline plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For inline plots within the notebook\n",
    "%matplotlib inline\n",
    "# Allows code cells to be intrepreted as R (put %%R on the first line) [^1]\n",
    "%load_ext rpy2.ipython\n",
    "# Render R output as HTML\n",
    "from functools import partial\n",
    "from rpy2.ipython import html\n",
    "html.html_rdataframe=partial(html.html_rdataframe, table_class=\"docutils\")\n",
    "html.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries for R session (see `readme.md` for installation instructions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(caret)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_state = numpy.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "Each row in the data represents a single TCP connection, as described in the original task description [^2]:\n",
    "> A connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol.  Each connection is labeled as either normal, or as an attack, with exactly one specific attack type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'connection_label']\n",
    "connection_events = pandas.read_csv('http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz', names=columns)  # [^3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_events.head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `connection_label` column describes the source of each connection; either the name of the red-team which caused the event, or the string `normal.` which indicates normal network behaviour. \n",
    "\n",
    "The task is to create a model which can separate red-team behavour from normal network behaviour, so group the data into two labels: `normal` and `bad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_label(connection_label):\n",
    "    return 'normal' if connection_label == 'normal.' else 'bad'\n",
    "\n",
    "connection_events['connection_label'] = connection_events['connection_label'].apply(func=generate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, separate out the labels from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection_labels = connection_events.filter(['connection_label'], axis='columns')\n",
    "connection_events = connection_events.drop(['connection_label'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data\n",
    "\n",
    "It is now necessary to split the data into training and testing sets. When doing this we should ask the following questions:\n",
    "\n",
    "  1. Does the ratio of normal and bad connections need to be similar in the training and testing data? If so, we should use stratified testing.\n",
    "    - Unless you include all types in the training set, the model won't necessarily be able to distinguish between the classes. Further, if we sample the incorrect ratio of classes in our training and testing sets, we have the potential to bias the model against rare classes.\n",
    "  2. Should we consider k-fold validation?\n",
    "    - What do we think? Yes: it helps to protect against overfitting of the data [^ref???] and allows us to better measure how our model will perform against unseen data.\n",
    "    - This is quick to implement, but would require each of us to write our models in such a way that they are repeatable.\n",
    "    \n",
    "**TODO:** If our models perform well, we should consider doing a quick test at the end with new data (from the non-10percent data set). This was mentioned as a potential think to do in the assessment spec:\n",
    "> ... for example, if you believe that you can predict new types of data, you could demonstrate this by leaving out some types of data and observing your performance.\n",
    "    \n",
    "Perform 10-fold cross-validation on the data, generating the `train_test_splits` variable mapping a training and testing set to each experiment. This process splits the data into ten bins (stratified to ensure a similar ratio of `normal` and `bad` connections in each group), then creates ten combinations of nine bins of training data to one bin of testing data. These are configured in such a way that each of the ten bins is used for testing at some point.\n",
    "\n",
    "By doing this, we have ensured that every single data point has been left out of training and tested against. This should help protect against overfitting of data, and allows us to better measure how our model will perform against unseen data. Further, by doing this ten times, we have lowered the potential for \"lucky dips\" when randomly sampling training and test data sets (in contrast to the leave-one-out methodology).\n",
    "\n",
    "When splitting data into training and testing sets, it is important to consider the ratio's of different classes. In our case, around $20\\%$ of our connection events are classified as `normal` (with the resr being classified as `bad`). We should also consider whether having unbalanced class ratios could have an effect on either the model, or our performance metrics. In the case of the model, this should not have an affect <insert intution here based on how logistic regression works> and these sources (or there sources):\n",
    "  - https://www.analyticbridge.datasciencecentral.com/forum/topics/handling-imbalanced-data-when-building-regression-models\n",
    "  - http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/\n",
    "  - https://stats.stackexchange.com/questions/6067/does-an-unbalanced-sample-matter-when-doing-logistic-regression . This looks particularly good, and makes a good point: our class ratio's do not reflect real world data. (I would imagine) that the vast majority of traffic experienced by an companies internal network is going to be `normal`. Our data set is the opposite (20% normal). Does this have the potential to affect our performance?\n",
    "    - In general, we should be aware that we are looking for *rare events*, and modify our analysis to represent this: https://statisticalhorizons.com/logistic-regression-for-rare-events\n",
    "   - **potential suggestion** use penalized likelihood!\n",
    "   - we should also talk about how the unrealistic data set has the potential to wrongly bias our models, and make them less applicable to real world scenarios.\n",
    "  \n",
    "If so, we should use stratified testing.\n",
    "    - Unless you include all types in the training set, the model won't necessarily be able to distinguish between the classes. Further, if we sample the incorrect ratio of classes in our training and testing sets, we have the potential to bias the model against rare classes.\n",
    "    \n",
    "The code below performs 10-fold cross-validation, providing the array `train_test_splits`. It does this using the [model selection](http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) part of the scikit-learn library [^5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold_splitter = model_selection.StratifiedKFold(n_splits=10,  random_state=random_state)\n",
    "train_test_splits = k_fold_splitter.split(\n",
    "    connection_events,  # data to be split\n",
    "    connection_labels,  # target/class to split by\n",
    ")\n",
    "\n",
    "# Force evaluation of the train_test_splits generator into a list. This needs to be\n",
    "# done before it's sent to R.\n",
    "train_test_splits = [\n",
    "    [training_indexes, testing_indexes]\n",
    "    for training_indexes, testing_indexes in train_test_splits\n",
    "]\n",
    "\n",
    "# Here train_test_splits is a list, where each item represents a single 90/10 split of \n",
    "# training and testing data respectively. The values contained are 0-based indexes\n",
    "# of the rows in each part of the split, i.e:\n",
    "#\n",
    "#   train_test_splits = [\n",
    "#      (indexes_of_training_samples, indexes_of_test_samples),  # first split\n",
    "#      (indexes_of_training_samples, indexes_of_test_samples),  # second split\n",
    "#      ...\n",
    "#      (indexes_of_training_samples, indexes_of_test_samples),  # kth split\n",
    "#  ]\n",
    "#\n",
    "# These indexes can then be used to fetch the data samples and their labels,\n",
    "# ready for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports the training and testing sets into the R session, ready for analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%Rpush train_test_splits connection_events connection_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are helper functions to extract the training and test data from a data frame using an item in the `train_test_splits` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "get_training_rows <- function(dataframe, train_test_split) {\n",
    "    # Training indexes are the first item in a train_test_split\n",
    "    indexes <- train_test_split[[1]]\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1.\n",
    "    # Correct for this by incrementing each index by 1:\n",
    "    indexes <- indexes + 1\n",
    "    \n",
    "    dataframe[indexes,]\n",
    "}\n",
    "\n",
    "get_testing_rows <- function(dataframe, train_test_split) {\n",
    "    # Testing indexes are the second item in a train_test_split\n",
    "    indexes <- train_test_split[[2]]\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1.\n",
    "    # Correct for this by incrementing each index by 1:\n",
    "    indexes <- indexes + 1  \n",
    "    \n",
    "    dataframe[indexes,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows how to extract the first set of training and testing data using the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Use the indexes to get out the training data\n",
    "training_data <- get_training_rows(connection_events, train_test_splits[[1]])\n",
    "training_labels <- get_training_rows(connection_labels, train_test_splits[[1]])\n",
    "\n",
    "# Use the indexes to get out the testing data\n",
    "testing_data <- get_testing_rows(connection_events, train_test_splits[[1]])\n",
    "testing_labels <- get_testing_rows(connection_labels, train_test_splits[[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model\n",
    "\n",
    "Here's a quick demo of using the data within the R session, and creating a model based upon it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%R head(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "qplot(x=src_bytes, y=dst_bytes, data=testing_data, geom='point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we would want to do the folllowing for each train/test split $i$ :\n",
    "  1. Train the model based on the $i$-th set of training data and labels.\n",
    "  2. Apply this trained model on the $i$-th set of testing data.\n",
    "  3. Save the set of predicted labels.\n",
    "  \n",
    "Then return a vector of each of these sets of predicted labels.\n",
    "\n",
    "For this example, come up with a silly set of predicted labels for each of our 10 training/testing splits. This is the format of predicted labels expected by the performance analsysi in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "predicted_label_sets <- list(\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[1]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[2]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[3]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[4]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[5]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[6]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[7]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[8]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[9]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[10]][[2]]), replace=TRUE, prob=c(0.5, 0.5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "  - Info on Binary Logistic Regression : https://www.statisticssolutions.com/binary-logistic-regression/\n",
    "  - Logit model in R: https://onlinecourses.science.psu.edu/stat504/node/225/\n",
    "  - Practical:\n",
    "    - https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\n",
    "    - https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py\n",
    "    - https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py\n",
    "    \n",
    "This plot might be useful for explanation: https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py\n",
    "  \n",
    " \n",
    "### Data Preparation\n",
    "Do I need to prepare the data in any way?\n",
    "  - **TODO** Transform categorical columns into numeric columns?\n",
    "  - **TODO** Normalization maybe?\n",
    "  - **TODO** More?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Columns\n",
    "\n",
    "Choose to do this using one-hot encoding as this avoids assigning implicit meaning or relationships between the categories. For example, if we naively assigned the categories in `protocol_type` the numbers `1,2,3`, we may end up implying to the model that $\\text{tcp} \\gt \\text{icmp}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_events.select_dtypes(include=[object]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So need to transform the `protocol_type`, `service` and `flag` columns. Consider their values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(connection_events.protocol_type), set(connection_events.service), set(connection_events.flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_events = pandas.get_dummies(connection_events, columns=[\"protocol_type\", \"service\", \"flag\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot-encoding performed by, `pandas.get_dummies`, transforms a single feature that can take $n$ values, into $n$ features that can take $2$ values. This can be seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_events.filter(regex='(flag|protocol_type|service).*', axis='columns').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs scikit-learn's [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) [^5] implementation on each of the train/test splits, saving it's predictions for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_predictions = []\n",
    "\n",
    "for training_indices, testing_indices in train_test_splits:                                      \n",
    "    training_data = connection_events.iloc[training_indices]\n",
    "    training_labels = connection_labels.iloc[training_indices]\n",
    "\n",
    "    logistic_regression = linear_model.LogisticRegression()\n",
    "    logistic_regression.fit(training_data, training_labels)\n",
    "    \n",
    "    testing_data = connection_events.iloc[testing_indices]\n",
    "    predicted_labels = logistic_regression.predict(testing_data)\n",
    "    \n",
    "    testing_predictions.append(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Describe scikit-learns logisitic regression functions, and how the differ from that which is described in a textbook:\n",
    "  1. They use L1 penalisation by default, what does that mean?\n",
    "  2. They try to find an optimal probability threshold automatically, how do they do it and why?\n",
    "    - Plot the probabilities with the computed threshold.\n",
    "  3. Did they make any other interesting choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "     \n",
    " ### Feature Selection    \n",
    "Feature selection:\n",
    "  - scikit-learn: https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "  - https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/\n",
    "  - Could do PCA/SVD and look at the first few principle components:\n",
    "    - their eigenvectors will each weight the original features by important.\n",
    "    - Using this we could either:\n",
    "      - do logistical regresion on the principle components,\n",
    "      - or, identify the most important features from the PCA.\n",
    "      - talked about here: https://stackoverflow.com/questions/34052115/how-to-find-the-importance-of-the-features-for-a-logistic-regression-model/34052747\n",
    "  - \"Selecting Features for Intrusion Detection: A Feature Relevance Analysis on KDD 99 Intrusion Detection Datasets\"\n",
    "     -  https://pdfs.semanticscholar.org/1d6e/a73b6e08ed9913d3aad924f7d7ced4477589.pdf\n",
    "     -  uses *binary discriminant analysis* to pick out features\n",
    "     \n",
    "     \n",
    "#### Why is it interesting?\n",
    "\n",
    "In real-world scenarios we will often want our classification system to work in real-time, identifying threats as soon as possible after they happen. Reducing the number of features has the potential to speed up the execution time of our classification algorithms, allowing new threats to be found and acted upon promptly.\n",
    "\n",
    "From \"Characterization and classification of malicious Web traffic\" [^8]:\n",
    ">  In addition to learning about characteristics of malicious activities, reducing the number of features by removing the irrelevant and noisy features speeds up the machine learning algorithms and hopefully improves their performance (Liu and Yu, 2005) [^9].\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using PCA to select rank feature relevance\n",
    "  - https://scikit-learn.org/stable/modules/decomposition.html#pca\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(\n",
    "    svd_solver='full',  # force sci-kit learn to compute the exact PCA rather than an estimation\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "pca.fit(connection_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc0 = pca.components_[0]\n",
    "sorted(zip(connection_events.columns, pc0), key=lambda pair: abs(pair[1]))[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc1 = pca.components_[1]\n",
    "sorted(zip(connection_events.columns, pc1), key=lambda pair: abs(pair[1]))[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc2 = pca.components_[2]\n",
    "sorted(zip(connection_events.columns, pc2), key=lambda pair: abs(pair[1]))[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above looks at the three most important principle components, and looks at which of the original features they each value. They are each > 99% comprised of a single feature. In order of importance: src_bytes, dst_bytes, duration.\n",
    "\n",
    "We should do a plot to find out how much each of the PC's contributes, then select the highly weighted features for each of the top-ranked PCs. Then:\n",
    "\n",
    "  1. Re-run the regression on only those features\n",
    "  2. Re-run the regression on the first few PC's\n",
    "  3. Do the reverse and find the least important features, then remove them from the data set and run it again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Additional Covariates\n",
    "\n",
    "\n",
    "### Logistic Regression with Penalized Likelihood\n",
    "\n",
    "\n",
    "### Another approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the predictions from the R session into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%Rpull predicted_label_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rpy2 returns an r-type list of character vectors. Convert each set of predictions into a numpy \n",
    "# array for processing with numpy/pandas/sklearn etc.\n",
    "predicted_label_sets = [numpy.array(predicted_labels) for predicted_labels in predicted_label_sets]\n",
    "\n",
    "## Do the above if we want to evaluate the R model, but we don't, evaluate the predictions from the Python model instead:\n",
    "predicted_label_sets = testing_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance analysis could be done in R or Python. Here is an example in Python [^4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_label_sets = [connection_labels.iloc[testing_indexes] for training_indexes, testing_indexes in train_test_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrixes = [\n",
    "    metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "    for true_labels, predicted_labels\n",
    "    in zip(true_label_sets, predicted_label_sets)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_confusion_matrix = sum(confusion_matrixes)\n",
    "summary_confusion_matrix = pandas.DataFrame(\n",
    "    data=summary_confusion_matrix, \n",
    "    index=['True Normal', 'True Bad'], \n",
    "    columns=['Predicted Normal', 'Predicted Bad'],\n",
    ")\n",
    "\n",
    "summary_confusion_figure, summary_confusion_axes = matplotlib.pyplot.subplots()\n",
    "summary_confusion_axes.set_title(\n",
    "    'Confusion matrix showing the predicted vs. true \\n'\n",
    "    'class of \"normal\" and \"bad\" network connections.'\n",
    ")\n",
    "seaborn.heatmap(\n",
    "    summary_confusion_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=seaborn.color_palette(\"Blues\"),\n",
    "    vmin=0,\n",
    "    ax=summary_confusion_axes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sensitivity(confusion_matrix):\n",
    "    true_positives = confusion_matrix['Predicted Normal']['True Normal']\n",
    "    false_negatives = confusion_matrix['Predicted Bad']['True Normal']\n",
    "    return true_positives/(true_positives+false_negatives)\n",
    "\n",
    "print('Sensitivity: {:.2f}%'.format(\n",
    "    sensitivity(summary_confusion_matrix)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def specificity(confusion_matrix):\n",
    "    false_positives = confusion_matrix['Predicted Normal']['True Bad']\n",
    "    true_negatives = confusion_matrix['Predicted Bad']['True Bad']\n",
    "    return true_negatives/(true_negatives+false_positives)\n",
    "\n",
    "print('Specificity: {:.2f}%'.format(\n",
    "    specificity(summary_confusion_matrix)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuprisingly, modelling the traffic as being uniformly distributed with a 50/50 split did not work particularly well. In fact, it gave sensitivity and specificity measures of approximately $50\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Approaches:\n",
    "  -  Info on PRESS(predicted residual error sum of squares): https://en.wikipedia.org/wiki/PRESS_statistic\n",
    "  - ROC Curve \n",
    "    - <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>\n",
    "    - Practical Statistics for Data Science, 1st ed., by Peter Bruce and Andrew Bruce (O’Reilly Media, 2017).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on real-world context\n",
    "\n",
    "The dataset contains $20\\%$ normal traffic and $80\\%$ malicious. This is consistent with recent real-world analysis on public-facing web services performed by Incapsula [^6]. Our use case, however, is intrusion detection within internal networks [^2], where malicious traffic should be a rarity.\n",
    "\n",
    "This means that  performance metric focused on the KDD Cup 99 data set may not be applicable to models used for real-world scenarios. In particular, an overly sensitive model would be favoured in the KDD Cup 99 data set, since its error would be smaller in ratio compared to a real world data set, which may have less than $5\\%$  malicious traffic internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The KDD Cup 99 data set [^3], being one of the few publically available data sets of this size and nature, has receieved a large amount of study and analysis within academic community. This has provided a pool of prior work to learn from. \n",
    "\n",
    "In particular, Katos 2007 [^7] evaluated three key approaches to detecting normal and malicious connections within the KDD 99 data set  - cluster, discriminant and logit analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[^1]: rpy2, https://rpy2.bitbucket.io/.\n",
    "\n",
    "[^2]: KDD-CUP-99 Task Description, http://kdd.ics.uci.edu/databases/kddcup99/task.html.\n",
    "\n",
    "[^3]: Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.\n",
    "\n",
    "[^4]: Data Science Toolbox: Assignment 1, https://github.com/dj311/data-science-toolbox-1.\n",
    "\n",
    "[^5]: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "[^6]: Incapsula: Annual Bot Traffic Report, https://www.incapsula.com/about/press-releases/incapsula-finds-malicious-bots-account-for-approximately-30-percent-of-internet-traffic/\n",
    "\n",
    "[^7]: Katos, V., 2007. Network intrusion detection: Evaluating cluster, discriminant, and logit analysis. Information Sciences, 177(15), pp.3060-3073. https://doi.org/10.1016/j.ins.2007.02.034\n",
    "\n",
    "[^8]: Goseva-Popstojanova, Katerina, et al. \"Characterization and classification of malicious Web traffic.\" computers & security 42 (2014): 92-115. https://doi.org/10.1016/j.cose.2014.01.006\n",
    "\n",
    "[^9]: Liu, Huan, and Lei Yu. \"Toward integrating feature selection algorithms for classification and clustering.\" IEEE Transactions on knowledge and data engineering 17.4 (2005): 491-502. https://doi.org/10.1109/TKDE.2005.66"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
