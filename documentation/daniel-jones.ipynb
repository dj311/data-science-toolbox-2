{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection on the KDD Cup 99 Data Set: Documentation\n",
    "*â€” Daniel Jones*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries for Python session (see `readme.md` for installation instructions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib  # graphs and plotting for python\n",
    "import numpy  # fast arrays for python (used by pandas)\n",
    "import pandas  # provides dataframe's and similar structures for python\n",
    "import seaborn  # provides pre-configured, pretty graphs for matplotlib\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup Jupyter with `rpy2` to allow embedding R, and `matplotlib` to allow inline plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For inline plots within the notebook\n",
    "%matplotlib notebook\n",
    "# Allows code cells to be intrepreted as R (put %%R on the first line) [^1]\n",
    "%load_ext rpy2.ipython\n",
    "# Render R output as HTML\n",
    "from rpy2.ipython import html\n",
    "html.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries for R session (see `readme.md` for installation instructions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "library(caret)\n",
    "library(ggplot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_state = numpy.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "Each row in the data represents a single TCP connection, as described in the original task description [^2]:\n",
    "> A connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol.  Each connection is labeled as either normal, or as an attack, with exactly one specific attack type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'connection_label']\n",
    "connection_events = pandas.read_csv('http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz', names=columns)  # [^3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_events.head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `connection_label` column describes the source of each connection; either the name of the red-team which caused the event, or the string `normal.` which indicates normal network behaviour. \n",
    "\n",
    "The task is to create a model which can separate red-team behavour from normal network behaviour, so group the data into two labels: `normal` and `bad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_label(connection_label):\n",
    "    return 'normal' if connection_label == 'normal.' else 'bad'\n",
    "\n",
    "connection_events['connection_label'] = connection_events['connection_label'].apply(func=generate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, separate out the labels from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "connection_labels = connection_events.filter(['connection_label'], axis='columns')\n",
    "connection_events = connection_events.drop(['connection_label'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data\n",
    "\n",
    "It is now necessary to split the data into training and testing sets. When doing this we should ask the following questions:\n",
    "\n",
    "  1. Does the ratio of normal and bad connections need to be similar in the training and testing data? If so, we should use stratified testing.\n",
    "    - Unless you include all types in the training set, the model won't necessarily be able to distinguish between the classes. Further, if we sample the incorrect ratio of classes in our training and testing sets, we have the potential to bias the model against rare classes.\n",
    "  2. Should we consider k-fold validation?\n",
    "    - What do we think? Yes: it helps to protect against overfitting of the data [^ref???] and allows us to better measure how our model will perform against unseen data.\n",
    "    - This is quick to implement, but would require each of us to write our models in such a way that they are repeatable.\n",
    "    \n",
    "**TODO:** If our models perform well, we should consider doing a quick test at the end with new data (from the non-10percent data set). This was mentioned as a potential think to do in the assessment spec:\n",
    "> ... for example, if you believe that you can predict new types of data, you could demonstrate this by leaving out some types of data and observing your performance.\n",
    "    \n",
    "Perform 10-fold cross-validation on the data, generating the `train_test_splits` variable mapping a training and testing set to each experiment. This process splits the data into ten bins (stratified to ensure a similar ratio of `normal` and `bad` connections in each group), then creates ten combinations of nine bins of training data to one bin of testing data. These are configured in such a way that each of the ten bins is used for testing at some point.\n",
    "\n",
    "By doing this, we have ensured that every single data point has been left out of training and tested against. This should help protect against overfitting of data, and allows us to better measure how our model will perform against unseen data. Further, by doing this ten times, we have lowered the potential for \"lucky dips\" when randomly sampling training and test data sets (in contrast to the leave-one-out methodology).\n",
    "\n",
    "When splitting data into training and testing sets, it is important to consider the ratio's of different classes. In our case, around $20\\%$ of our connection events are classified as `normal` (with the resr being classified as `bad`). We should \n",
    "\n",
    "If so, we should use stratified testing.\n",
    "    - Unless you include all types in the training set, the model won't necessarily be able to distinguish between the classes. Further, if we sample the incorrect ratio of classes in our training and testing sets, we have the potential to bias the model against rare classes.\n",
    "    \n",
    "The code below performs 10-fold cross-validation, providing the array `train_test_splits`. It does this using the [model selection](http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) part of the scikit-learn library [^5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_fold_splitter = model_selection.StratifiedKFold(n_splits=10,  random_state=random_state)\n",
    "train_test_splits = k_fold_splitter.split(\n",
    "    connection_events,  # data to be split\n",
    "    connection_labels,  # target/class to split by\n",
    ")\n",
    "\n",
    "# Force evaluation of the train_test_splits generator into a list. This needs to be\n",
    "# done before it's sent to R.\n",
    "train_test_splits = [\n",
    "    [training_indexes, testing_indexes]\n",
    "    for training_indexes, testing_indexes in train_test_splits\n",
    "]\n",
    "\n",
    "# Here train_test_splits is a list, where each item represents a single 90/10 split of \n",
    "# training and testing data respectively. The values contained are 0-based indexes\n",
    "# of the rows in each part of the split, i.e:\n",
    "#\n",
    "#   train_test_splits = [\n",
    "#      (indexes_of_training_samples, indexes_of_test_samples),  # first split\n",
    "#      (indexes_of_training_samples, indexes_of_test_samples),  # second split\n",
    "#      ...\n",
    "#      (indexes_of_training_samples, indexes_of_test_samples),  # kth split\n",
    "#  ]\n",
    "#\n",
    "# These indexes can then be used to fetch the data samples and their labels,\n",
    "# ready for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports the training and testing sets into the R session, ready for analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%Rpush train_test_splits connection_events connection_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are helper functions to extract the training and test data from a data frame using an item in the `train_test_splits` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "get_training_rows <- function(dataframe, train_test_split) {\n",
    "    # Training indexes are the first item in a train_test_split\n",
    "    indexes <- train_test_split[[1]]\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1.\n",
    "    # Correct for this by incrementing each index by 1:\n",
    "    indexes <- indexes + 1\n",
    "    \n",
    "    dataframe[indexes,]\n",
    "}\n",
    "\n",
    "get_testing_rows <- function(dataframe, train_test_split) {\n",
    "    # Testing indexes are the second item in a train_test_split\n",
    "    indexes <- train_test_split[[2]]\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1.\n",
    "    # Correct for this by incrementing each index by 1:\n",
    "    indexes <- indexes + 1  \n",
    "    \n",
    "    dataframe[indexes,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows how to extract the first set of training and testing data using the functions above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# Use the indexes to get out the training data\n",
    "training_data <- get_training_rows(connection_events, train_test_splits[[1]])\n",
    "training_labels <- get_training_rows(connection_labels, train_test_splits[[1]])\n",
    "\n",
    "# Use the indexes to get out the testing data\n",
    "testing_data <- get_testing_rows(connection_events, train_test_splits[[1]])\n",
    "testing_labels <- get_testing_rows(connection_labels, train_test_splits[[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model\n",
    "\n",
    "Here's a quick demo of using the data within the R session, and creating a model based upon it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "summary(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "qplot(x=src_bytes, y=dst_bytes, data=testing_data, geom='point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we would want to do the folllowing for each train/test split $i$ :\n",
    "  1. Train the model based on the $i$-th set of training data and labels.\n",
    "  2. Apply this trained model on the $i$-th set of testing data.\n",
    "  3. Save the set of predicted labels.\n",
    "  \n",
    "Then return a vector of each of these sets of predicted labels.\n",
    "\n",
    "For this example, come up with a silly set of predicted labels for each of our 10 training/testing splits. This is the format of predicted labels expected by the performance analsysi in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "predicted_label_sets <- list(\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[1]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[2]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[3]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[4]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[5]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[6]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[7]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[8]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[9]][[2]]), replace=TRUE, prob=c(0.5, 0.5)),\n",
    "    sample(c('normal', 'bad'), nrow(train_test_splits[[10]][[2]]), replace=TRUE, prob=c(0.5, 0.5))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the predictions from the R session into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%Rpull predicted_label_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rpy2 returns an r-type list of character vectors. Convert each set of predictions into a numpy \n",
    "# array for processing with numpy/pandas/sklearn etc.\n",
    "predicted_label_sets = [numpy.array(predicted_labels) for predicted_labels in predicted_label_sets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance analysis could be done in R or Python. Here is an example in Python [^4]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_label_sets = [connection_labels.iloc[testing_indexes] for training_indexes, testing_indexes in train_test_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrixes = [\n",
    "    metrics.confusion_matrix(true_labels, predicted_labels)\n",
    "    for true_labels, predicted_labels\n",
    "    in zip(true_label_sets, predicted_label_sets)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_confusion_matrix = sum(confusion_matrixes)\n",
    "summary_confusion_matrix = pandas.DataFrame(\n",
    "    data=summary_confusion_matrix, \n",
    "    index=['True Normal', 'True Bad'], \n",
    "    columns=['Predicted Normal', 'Predicted Bad'],\n",
    ")\n",
    "\n",
    "summary_confusion_figure, summary_confusion_axes = matplotlib.pyplot.subplots()\n",
    "summary_confusion_axes.set_title(\n",
    "    'Confusion matrix showing the predicted vs. true \\n'\n",
    "    'class of \"normal\" and \"bad\" network connections.'\n",
    ")\n",
    "seaborn.heatmap(\n",
    "    summary_confusion_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=seaborn.color_palette(\"Blues\"),\n",
    "    vmin=0,\n",
    "    ax=summary_confusion_axes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sensitivity(confusion_matrix):\n",
    "    true_positives = confusion_matrix['Predicted Normal']['True Normal']\n",
    "    false_negatives = confusion_matrix['Predicted Bad']['True Normal']\n",
    "    return true_positives/(true_positives+false_negatives)\n",
    "\n",
    "print('Sensitivity: {:.2f}%'.format(\n",
    "    sensitivity(summary_confusion_matrix)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def specificity(confusion_matrix):\n",
    "    false_positives = confusion_matrix['Predicted Normal']['True Bad']\n",
    "    true_negatives = confusion_matrix['Predicted Bad']['True Bad']\n",
    "    return true_negatives/(true_negatives+false_positives)\n",
    "\n",
    "print('Specificity: {:.2f}%'.format(\n",
    "    specificity(summary_confusion_matrix)*100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsuprisingly, modelling the traffic as being uniformly distributed with a 50/50 split did not work particularly well. In fact, it gave sensitivity and specificity measures of approximately $50\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[^1]: rpy2, https://rpy2.bitbucket.io/.\n",
    "\n",
    "[^2]: KDD-CUP-99 Task Description, http://kdd.ics.uci.edu/databases/kddcup99/task.html.\n",
    "\n",
    "[^3]: Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.\n",
    "\n",
    "[^4]: Data Science Toolbox: Assignment 1, https://github.com/dj311/data-science-toolbox-1.\n",
    "\n",
    "[^5]: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
