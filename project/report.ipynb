{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection on the KDD Cup 99 Data Set\n",
    "*An investigation into models and performance metrics for the classification of network data.*\n",
    "\n",
    "--- \n",
    "\n",
    "**TODO** Standardize on what we consider a positive and negative result e.g. is normal positive, or is bad positive? This will ensure our write-ups and code are consistent and make sense together\n",
    "  - Positive :: detection of intrusion (`bad.` label)\n",
    "  - Negative :: detected as normal traffic (`normal.` label)\n",
    "\n",
    "**TODO** Use the same labels for normal and bad data. We've decided on `normal.` and `bad.`.\n",
    "\n",
    "**TODO** Introduction. Should include:\n",
    "\n",
    "  * Introduce the KDD Cup:\n",
    "    - what was it's aims?\n",
    "    - what does the data set look like? features etc.\n",
    "  * Prior work on the kdd 99 data set\n",
    "    - extensively studied\n",
    "    - papers, code we found, etc.\n",
    "  * Introduce our intended approach, talk about:\n",
    "    - choose a number of approaches to compare\n",
    "    - lots of thought has gone into our performance metric <-- make this clear\n",
    "    \n",
    "We decided to consider three approaches to this problem - logistic regression, logistic regression with penalisation, and support vector machines. We have allocated the work as follows:\n",
    "  1. Shanglin will work on the logistic regression model.\n",
    "  2. Daniel will work on an extension of logistic regression using penalisation and feature selection.\n",
    "  3. Kishalay will work on a model using support vector machines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook needs to be setup with the required libraries and dependencies loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from statsmodels import api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inline plots within the notebook\n",
    "%matplotlib inline\n",
    "# Allows code cells to be intrepreted as R (put %%R on the first line) [1]\n",
    "%load_ext rpy2.ipython\n",
    "# Render R output as HTML\n",
    "from functools import partial\n",
    "from rpy2.ipython import html\n",
    "html.html_rdataframe=partial(html.html_rdataframe, table_class=\"docutils\")\n",
    "html.init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(data.table)\n",
    "library(lattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed the R and Python random number generators to ensure we get consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = numpy.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "This project uses the same dataset as was originally used in the 1999 KDD Intrusion Detection Contest [2] [3]. The contest aim was to survey and evaluate research in intrusion detection. To do this, an environment was created to simulate the conditions of a typical U.S. Air Force intranet, with the addition malicious traffic. \n",
    "\n",
    "The dataset is structured such that each row represents a single Transmission Control Protocol (TCP) connection, summarising a sequence of packets between a source and destination computer within the network. The code below loads in this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'connection_label']\n",
    "connection_events = pandas.read_csv('../data/kddcup.data_10_percent.gz', names=columns)  # sourced from http://kdd.ics.uci.edu/ [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row has a `connection_label` feature, describing the source of each connection; either an identifier of the attack-type it is associated with, or the string `normal.` in the case of normal network traffic.\n",
    "\n",
    "Since the task is to separate malicious behavour from normal network behaviour, the code below separates these labels from the rest of the dataset, then groups all malicious traffic under the label `bad.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connection_labels = connection_events.filter(['connection_label'], axis='columns')\n",
    "connection_events = connection_events.drop(['connection_label'], axis='columns')\n",
    "\n",
    "def generate_label(label):\n",
    "    return 'normal.' if label == 'normal.' else 'bad.'\n",
    "\n",
    "connection_labels = connection_labels.apply(func=generate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** If we have time, should we include some EDA here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Classification\n",
    "From spec:\n",
    "> - Together agree and test a performance metric.\n",
    ">   - Half of the effort should be devoted to exploring appropriate performance measures.\n",
    ">   - You should create a test and validation dataset, but you may choose how to do this.\n",
    "\n",
    "\n",
    "From spec:\n",
    "> * Think about the circumstances by which your chosen performance metric\n",
    "will lead to real-world generalisability, and how it might compromise this for\n",
    "the purpose of standardization.\n",
    "> * Demonstrate this with data and/or simulation;\n",
    "for example, if you believe that you can predict new types of data, you could\n",
    "demonstrate this by leaving out some types of data and observing your perfor-\n",
    "mance. \n",
    "> * Examine in what sense your group’s best method is truly best.\n",
    "\n",
    "### Cross-Validation (TODO: Kish)\n",
    "  - Part of conclusion should say: \"We consider cross-validation an important part of the performance metric, since it increases the accuracy in the summary statistics that we may choose for the final comparisons\".\n",
    "    - i.e. indicate the this work should be assessed as part of the performance metrics\n",
    "  - Decided on k-fold, why? Brief comparison to other types.\n",
    "  \n",
    "\n",
    "The code below performs 10-fold cross-validation, providing the array `train_test_splits`. It does this using the [model selection](http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) part of the scikit-learn library [5]. Stratified k-fold validation was chosen to preserve the normal/bad class ratios between the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_fold_splitter = model_selection.StratifiedKFold(n_splits=10,  random_state=random_state)\n",
    "train_test_splits = k_fold_splitter.split(\n",
    "    connection_events,  # data to be split\n",
    "    connection_labels,  # target/class to split by\n",
    ")\n",
    "\n",
    "# Force evaluation of the train_test_splits generator into a list. This needs to be\n",
    "# done before it's sent to R.\n",
    "train_test_splits = [\n",
    "    [training_indices, testing_indices]\n",
    "    for training_indices, testing_indices in train_test_splits\n",
    "]\n",
    "\n",
    "# Here train_test_splits is a list, where each item represents a single 90/10 split of \n",
    "# training and testing data respectively. The values contained are 0-based indices\n",
    "# of the rows in each part of the split, i.e:\n",
    "#\n",
    "#   train_test_splits = [\n",
    "#      (indices_of_training_samples, indices_of_test_samples),  # first split\n",
    "#      (indices_of_training_samples, indices_of_test_samples),  # second split\n",
    "#      ...\n",
    "#      (indices_of_training_samples, indices_of_test_samples),  # kth split\n",
    "#  ]\n",
    "#\n",
    "# These indices can then be used to fetch the data samples and their labels,\n",
    "# ready for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports the training and testing sets into the R session, ready for analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Rpush train_test_splits connection_events connection_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions below extract training and test rows from a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "get_training_rows <- function(dataframe, train_test_split) {\n",
    "    indices <- train_test_split[[1]]  # Training indices are the first item in a train_test_split\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1. Correct for this by incrementing each index by 1:\n",
    "    indices <- indices + 1\n",
    "    \n",
    "    dataframe[indices,]\n",
    "}\n",
    "\n",
    "get_testing_rows <- function(dataframe, train_test_split) {\n",
    "    indices <- train_test_split[[2]]  # Testing indexes are the second item in a train_test_split\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1. Correct for this by incrementing each index by 1:\n",
    "    indices <- indices + 1  \n",
    "    \n",
    "    dataframe[indices,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (TODO: Shanglin)\n",
    "- confusion matrix is a table that contains information about actual and predicted classification. The Table below shows a table of confusion with 2 class classifer.\n",
    "- It is 2X2 table and we have four cells. Each cell represents different numbers:\n",
    "\n",
    " 1. True Positive(TP): Actual positive condition predicts as positve.\n",
    " 2. True Negative(TN): Actual negative condition predicts as negative.\n",
    " 3. Flase Postive(FP)(Type I Error): Actual negative condition predicts as positive.\n",
    " 4. False Negative(FN)(Type II Error): Actual Positive condition predicts as negative.\n",
    "\n",
    "- Confusion matrix is an obvious and easy performance metric to compare our models. Using the information in the confusion matrix to calculate some Statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionmatrix = pandas.DataFrame({\" \":[\"Predicted Positive\",\"Predicted Negative\"], \"Actual Postive\":[\"True Positive\",\"False Negative(Type II Error)\"], \"Actual Negative\":[\"Flase Postive(Type I Error)\",\"True Negative\"]})\n",
    "confusionmatrix.set_index([\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Our topic, our confusion matrix should be like this \n",
    "confusionmatrix = pandas.DataFrame({\" \":[\"Predicted Bad\",\"Predicted Normal\"], \"Actual bad\":[\"\",\"\"], \"Actual normal\":[\"\",\"\"]})\n",
    "confusionmatrix.set_index([\" \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics (TODO: Shanglin, Dan, Kish)\n",
    "\n",
    "Look here for table with lots of them on: https://en.wikipedia.org/wiki/Sensitivity_and_specificity and here is a comparison of two new ones: http://standardwisdom.com/softwarejournal/2011/12/matthews-correlation-coefficient-how-well-does-it-do/\n",
    "\n",
    "Based on confusion matrix, we can have many statistics to do:\n",
    "\n",
    "  - Accuracy: (TP+TN)/(TP+TN+FP+FN) is the proportion of the total number of predictions that were true.\n",
    "  - Sentitivity: TP/(TP+FN) is the proportion of positive condition that were correctly predicted.\n",
    "  - Specificity: TN/(TN+FP) is the proportion of negative condition that were correctly predicted.\n",
    "\n",
    "For our topic, accuracy is not a good performance metric since the dataset is unblanced. i.e \"bad\" has large proportion(80%). For exmaple, if prediction of models are all bad, we still have 80% accuracy, but have 0 sentitivity, which is not good. So sentitivity and specificity is somehow good for our models. Also, we have other metrics:\n",
    "  - ROC curve: is a plot with the false positive rate on the X axis and the true positive rate on the Y axis.\n",
    "    - The ROC curve allows us to consider the trade-off between the sensitivity and specificity of a model. Unfortunately, it is hard for us to choose in this setting because it does not produce a single value with which models can be compared.\n",
    "  - Kappa Statistics: \n",
    "  \n",
    "  \n",
    "  - Accuracy:\n",
    "     - Using a simple summary measure such as accuracy can cause us to optimise away the prediction of rare classes [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    true_positives = confusion_matrix[0][0]\n",
    "    true_negative = confusion_matrix[1][1]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    false_negative = confusion_matrix[1][0]\n",
    "    return (true_positives +true_negative)/(true_positives+false_positive+true_negatives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Sensitivity\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(confusion_matrix):\n",
    "    true_positives = confusion_matrix[0][0]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    return true_positives/(true_positives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Specificity\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(confusion_matrix):\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    true_negatives = confusion_matrix[1][1]\n",
    "    return true_negatives/(true_negatives+false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Kappa  (https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english). Here is an article on the kappa statistic: https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/\n",
    "    - The Kappa statistic is useful since it is a function of all elements of the confusion matrix. This has the potential to give a more balanced view of performance than using a metric like sensitivity on it's own.\n",
    "    - We've included an implementation of the Cohens' Kappa statistic in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kappa(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates the Kappa summary statistic [5] on the confusion matrix given. \n",
    "    \n",
    "    The confusion matrix should have the following structure:\n",
    "        [\n",
    "            [true_positives, false_positives],\n",
    "            [false_negatives, true_negatives],\n",
    "        ]\n",
    "    \"\"\"\n",
    "    true_positives = confusion_matrix[0][0]\n",
    "    true_negatives= confusion_matrix[1][1]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    \n",
    "    total = true_positives+false_positives+true_negatives+false_negatives\n",
    "    \n",
    "    observed_accuracy = (true_positives + true_negatives) / total\n",
    "    random_accuracy = (\n",
    "        (true_negatives+false_positives)*(true_negatives+false_negatives) \n",
    "        + (false_negatives+true_positives)*(false_positives+true_positives)\n",
    "    ) / (\n",
    "        total * total\n",
    "    )\n",
    "    \n",
    "    return (observed_accuracy - random_accuracy) / (1 - random_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "\n",
    "**TODO** Dan\n",
    "\n",
    "From spec:\n",
    "> For example, you could look to predict the next event on each\n",
    "edge based on past events on this edge, or you could model the network at a\n",
    "more global level, and many other approaches are possible.\n",
    "\n",
    "**TODO**: We decided to model it on a global scale, why?\n",
    "   - Real-llfe it is hard to keep track of state between events (especially true for high-traffic websites), it's easier to classify each incoming event individually as it goes with a previously trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (TODO: Shanglin)\n",
    "**TODO** k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "kddata<-read.csv(\"../data/kddcup.data_10_percent.gz\")\n",
    "\n",
    "kddnames=read.table(\n",
    "    \"../data/kddcup.names\",\n",
    "    sep=\":\",\n",
    "    skip=1,\n",
    "    as.is=T\n",
    ")\n",
    "\n",
    "colnames(kddata)=c(kddnames[,1], \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#set normal col to two levels:normal=0,non-normal=1\n",
    "levels(kddata$normal)[which(levels(kddata$normal) !='normal.')] <- \"non-normal.\"\n",
    "kddata$normal<-as.numeric(factor(kddata$normal,levels = c(\"normal.\",\"non-normal.\"))) -1 \n",
    "\n",
    "kddata_normal<-kddata[which(kddata$normal==0),]\n",
    "kddata_nonnormal<-kddata[which(kddata$normal==1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#set service col to three levels:private,http,others\n",
    "levels(kddata$service)[which(levels(kddata$service) != \"http\" & levels(kddata$service) != \"private\")] <- \"others\"\n",
    "kddata$service<-as.numeric(factor(kddata$service,levels = c(\"private\",\"http\",\"others\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#train and test\n",
    "kddata_normal<-kddata[which(kddata$normal==0),]\n",
    "kddata_nonnormal<-kddata[which(kddata$normal==1),]\n",
    "\n",
    "sample_normal<-sample(seq_len(nrow(kddata_normal)),size=floor(.80*nrow(kddata_normal)))\n",
    "sample_nonnormal<-sample(seq_len(nrow(kddata_nonnormal)),size=floor(.80*nrow(kddata_nonnormal)))\n",
    "\n",
    "train_normal<-kddata_normal[sample_normal,]\n",
    "test_normal<-kddata_normal[-sample_normal,]\n",
    "\n",
    "train_nonnormal<-kddata_nonnormal[sample_nonnormal,]\n",
    "test_nonnormal<-kddata_nonnormal[-sample_nonnormal,]\n",
    "\n",
    "train<-rbind(train_normal,train_nonnormal)\n",
    "test<-rbind(test_normal,test_nonnormal)\n",
    "\n",
    "head(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#fit in logit model\n",
    "model1<-glm(normal~service+logged_in+srv_count, family=binomial(link='logit'), data=train)\n",
    "\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "fitted.results <- predict(model1,newdata=test,type = 'response')\n",
    "fitted.results <- ifelse(fitted.results > 0.5,1,0)\n",
    "confusion<-table(fitted.results,test$normal)\n",
    "accuracy<-(confusion[1,1]+confusion[2,2])/length(fitted.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Penalisation (TODO: Dan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE (TODO: Kish) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(data.table)\n",
    "kddata = read.csv(\"../data/kddcup.data_10_percent.gz\")\n",
    "kddnames = read.table(\"../data/kddcup.names\",sep = \":\", skip = 1, as.is = T)\n",
    "colnames(kddata)=c(kddnames[,1],\"normal\")\n",
    "kddata\n",
    "length(kddata$normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "colcounts = lapply(unique_temp,length)\n",
    "constants = colcounts==1\n",
    "constants\n",
    "not_constants = !constants\n",
    "not_constants\n",
    "pruned_kddata = kddata[,not_constants]\n",
    "pruned_kddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "levels(pruned_kddata$normal) <- c(levels(pruned_kddata$normal), 'bad.')\n",
    "pruned_kddata[which(pruned_kddata$normal!='normal.'), 'normal'] <- 'bad.'\n",
    "pruned_kddata[which(pruned_kddata$normal!='normal.'),]\n",
    "pruned_kddata$normal = factor(pruned_kddata$normal, levels = c('normal.','bad.'))\n",
    "pruned_kddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dataset1 = pruned_kddata[pruned_kddata$normal==\"normal.\",]\n",
    "dataset1\n",
    "dataset2 = pruned_kddata[pruned_kddata$normal==\"bad.\",]\n",
    "dataset2\n",
    "length(dataset2$normal)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "sample1_indexes = sample(nrow(dataset1), size = floor(0.8*nrow(dataset1)), prob=NULL)\n",
    "sample1 = dataset1[sample1_indexes,]\n",
    "sample1\n",
    "length((sample1$normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "sample2_indexes = sample(nrow(dataset2), size = floor(0.2*nrow(dataset2)),prob=NULL)\n",
    "sample2 = dataset2[sample2_indexes,]\n",
    "sample2\n",
    "length((sample2$normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "main_sample1 = rbind(sample1,sample2)\n",
    "main_sample1\n",
    "test_data1 = dataset1[-sample1_indexes,]\n",
    "test_data1\n",
    "length(test_data1$normal)\n",
    "test_data2 = dataset2[-sample2_indexes,]\n",
    "length(test_data2$normal)\n",
    "main_test_data = rbind(test_data1,test_data2)\n",
    "main_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages('e1071')\n",
    "library(e1071)\n",
    "classifier = svm(formula = normal ~ ., data = main_sample1, type = \"C-classification\", kernel = \"linear\")\n",
    "\n",
    "y_pred = predict(classifier, newdata = main_test_data)\n",
    "table(main_test_data$normal, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages('caret')\n",
    "library(caret)\n",
    "library(e1071)\n",
    "folds = createFolds(main_sample1$normal, k = 10)\n",
    "cv = lapply(folds, function(x) {\n",
    "  training_fold = main_sample1[-x, ]\n",
    "  test_fold = main_sample1[x, ]\n",
    "  classifier = svm(formula = normal ~ ., data = training_fold, type = \"C-classification\", kernel = \"linear\")\n",
    "  y_pred = predict(classifier, newdata = test_fold)\n",
    "  cm = table(test_fold$normal, y_pred)\n",
    "  accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])\n",
    "  return(accuracy)\n",
    "  })\n",
    "cv\n",
    "Macc = mean(as.numeric(cv))\n",
    "Macc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "### Plan\n",
    "\n",
    "Here we apply performance metric to models.\n",
    "\n",
    "Create a summary table with:\n",
    "  - model\n",
    "  - summary statistic \n",
    "    - maybe multiple?\n",
    "    \n",
    "Talk about how some models performed better under different conditions.\n",
    "\n",
    "How applicable is each model for use in live intrusion detection in e.g. a corporate internal network.\n",
    "\n",
    "\n",
    "### Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Performance Metric\n",
    "\n",
    "### Plan\n",
    "Here we evaluate how good our performance metric was.\n",
    "\n",
    "Key points:\n",
    "  - Limitations of our performance metric:\n",
    "    - In the end, in order to pick a \"winner\" we need to decide on a single summary statistic. This approach to picking a model then becomes brute force and has loses the subtle differences in the models.\n",
    "    - The class ratios in our dataset do not match real-world ratios, this means our performance metric may overly value certain criteria over others. (TODO: this needs to be more concrete).\n",
    "    - Our performance metric in unable to take into account the conditions that an intrusion detection system would need to run under. Bla bla bla theres always a trade-off between model performance and speed.\n",
    "\n",
    "### Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Potential Extension: Compliance Budget\n",
    "\n",
    "**TODO** Dan: Talk about compliance budget [7] and it's relation to security, and the trade-off between false negative vs false positives. The compliance budget is the amount of time and effort a person is willing to put into an endeavor before their perceived benefit no longer outweighs the\n",
    "\n",
    "Actual calculations we can do:\n",
    "  - How many tcp connections are there per day?\n",
    "  - How many false alarms do we believe are acceptable per day before the users give and assume they are all false alarms?\n",
    "  - Given a false positive rate, we can calculate how many false positives each model would give per day. \n",
    "  - Restrain our models so that all must give below the false positives threshold, then rank them on their sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]: rpy2, https://rpy2.bitbucket.io/.\n",
    "\n",
    "[2]: KDD-CUP-99 Task Description, http://kdd.ics.uci.edu/databases/kddcup99/task.html.\n",
    "\n",
    "[3]: Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.\n",
    "\n",
    "[4]: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "[5]: Confusion Matrix – Another Single Value Metric – Kappa Statistic, https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/\n",
    "\n",
    "[6]: Practical Statistics for Data Science, 1st ed., by Peter Bruce and Andrew Bruce (O’Reilly Media, 2017).\n",
    "\n",
    "[7]: Beautement, Adam, M. Angela Sasse, and Mike Wonham. \"The compliance budget: managing security behaviour in organisations.\" Proceedings of the 2008 New Security Paradigms Workshop. ACM, 2009.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
