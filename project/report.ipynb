{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection on the KDD Cup 99 Data Set\n",
    "*An investigation into models and performance metrics for the classification of network data.*\n",
    "\n",
    "--- \n",
    "\n",
    "~~**TODO** Standardize on what we consider a positive and negative result e.g. is normal positive, or is bad positive? This will ensure our write-ups and code are consistent and make sense together~~\n",
    "  ~~- Positive :: detection of intrusion (`bad.` label)~~\n",
    "  ~~- Negative :: detected as normal traffic (`normal.` label)~~\n",
    "\n",
    "~~**TODO** Use the same labels for normal and bad data. We've decided on `normal.` and `bad.`.~~\n",
    "\n",
    "#### Plan\n",
    "\n",
    "  * Introduce the KDD Cup:\n",
    "    - what was it's aims?\n",
    "    - ~~what does the data set look like? features etc.~~ covered in \"data source\"\n",
    "  * Prior work on the kdd 99 data set\n",
    "    - extensively studied\n",
    "    - papers, code we found, etc.\n",
    "  * Introduce our intended approach, talk about:\n",
    "    - choose a number of approaches to compare\n",
    "    - lots of thought has gone into our performance metric <-- make this clear\n",
    "    \n",
    "The KDD Cup 99 data set [3], being one of the few publically available data sets of this size and nature, has receieved a large amount of study and analysis within academic community. This has provided a pool of prior work to learn from.\n",
    "\n",
    "In particular, Katos 2007 [15] evaluated three key approaches to detecting normal and malicious connections within the KDD 99 data set  - cluster, discriminant and logit analysis. Their results showed that... This informed our approach by...\n",
    "    \n",
    "We decided to consider three approaches to this problem - logistic regression, logistic regression with penalisation, and support vector machines. We have allocated the work as follows:\n",
    "  1. Shanglin will work on the logistic regression model.\n",
    "  2. Daniel will work on an extension of logistic regression using penalisation and feature selection.\n",
    "  3. Kishalay will work on a model using support vector machines.\n",
    "\n",
    "It is important to carefully consider the available performance metrics and how they apply to our models. In particular, we should evaluate the practical applicability of our models to intrusion detection systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook needs to be setup with the required libraries and dependencies loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed the R and Python random number generators to ensure we get consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = numpy.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source\n",
    "\n",
    "This project uses the same dataset as was originally used in the 1999 KDD Intrusion Detection Contest [2] [3]. The contest aim was to survey and evaluate research in intrusion detection. To do this, an environment was created to simulate the conditions of a typical U.S. Air Force intranet, with the addition malicious traffic. \n",
    "\n",
    "The dataset is structured such that each row represents a single Transmission Control Protocol (TCP) connection, summarising a sequence of packets between a source and destination computer within the network. The code below loads in this data set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row has in the original dataset has beel labelled, describing the source of each connection. This is either an identifier of the attack-type it is associated with, or the string `normal.` in the case of non-malicious network traffic.\n",
    "\n",
    "Since the task is to separate malicious behavour from normal network behaviour, we have decided to group each connection into one of two groups: `normal.` and `bad.`. This should help us to develop a standardised, consistent performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Classification\n",
    "From spec:\n",
    "> - Together agree and test a performance metric.\n",
    ">   - Half of the effort should be devoted to exploring appropriate performance measures.\n",
    ">   - You should create a test and validation dataset, but you may choose how to do this.\n",
    "> \n",
    "> * Think about the circumstances by which your chosen performance metric\n",
    "will lead to real-world generalisability, and how it might compromise this for\n",
    "the purpose of standardization.\n",
    "> * Demonstrate this with data and/or simulation;\n",
    "for example, if you believe that you can predict new types of data, you could\n",
    "demonstrate this by leaving out some types of data and observing your perfor-\n",
    "mance. \n",
    "> * Examine in what sense your group’s best method is truly best.\n",
    "It is important to carefully consider different performance metrics in relation to the application of our models, before choosing what we consider to be the most appropriate metric. In this section we compare a number of metrics.\n",
    "\n",
    "### Cross-Validation \n",
    "  Cross validation is a model validation technique for assessing how the results of a statistical analysis generalize to an independent dataset. It is used in prediction, and for checking the practical accuracy of the predicted model. In a typical problem, the dataset on hand is divided into a ‘Training Set’, and a ‘Test Set’. The model is then ‘trained’ on the Training Set. The effectiveness of the model is then tested using the Test Set, which is typically that part of the data which had not been fed into the model. This allows us to test the accuracy of the model in predicting observations which hadn’t been used to estimate it.\n",
    "  \n",
    "However, results obtained from a single round of cross validation may contain variability issues. Thus, to smooth out the effect of variability on the sampling (for the Test and Training sets), cross validation is usually performed for a large number of times. This results in a large number of accuracy values, which are then averaged. This mean value is then taken as the indication of the model’s predictive power, and is a better estimate than that obtained after a single iteration. There are a variety of methods by which this technique is implemented. In our project, we have used K Fold Cross Validation. \n",
    "\n",
    "In K Fold Cross Validation, the dataset is randomly divided into K equal sized samples. The model is trained on (K-1) samples, and the remaining sample is then used for testing it. This process is repeated K times, which ensures that every sample is used as the Test Data exactly once, and that all observations are used for training and testing. The mean of the values of accuracy for all the iterations is then reported. This method does a good job of reducing variability bias. \n",
    "\n",
    "Since K Fold Cross Validation allows us to obtain ‘better’ (less biased or variable) estimates of the accuracy of our model, we consider it to be an useful Performance Metric for the comparison of our models. \n",
    "\n",
    "\n",
    "The code below performs 10-fold cross-validation, providing the array `train_test_splits`. It does this using the [model selection](http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) part of the scikit-learn library [5]. Stratified k-fold validation was chosen to preserve the normal/bad class ratios between the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Confusion matrix is a table that contains information about actual and predicted classification. The Table below shows a table of confusion with 2 class classifer. The confusion matrix is represented by a matrix which each row represents the instances in a predicted class, while each column represents in an actual class. It is 2X2 table and we have four cells. Each cell represents different numbers:\n",
    "\n",
    " 1. True Positive(TP): Actual positive condition predicts as positve.\n",
    " 2. True Negative(TN): Actual negative condition predicts as negative.\n",
    " 3. Flase Postive(FP)(Type I Error): Actual negative condition predicts as positive.\n",
    " 4. False Negative(FN)(Type II Error): Actual Positive condition predicts as negative.\n",
    "\n",
    "The confusion matrix shows the ways in which your classification model is confused when it makes predictions. Confusion matrix is an obvious and easy performance metric to compare our models. We can use the information in the confusion matrix to calculate some statistics such as accuracy, sensitivity, etc. Those statistics can lead us a direct insight of performance of our model. \n",
    "\n",
    "Thus, confusion matrix is what we choose for our performance tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Negative</th>\n",
       "      <th>Actual Positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Negative</th>\n",
       "      <td>True Negative</td>\n",
       "      <td>Flase Negative(Type II Error)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Positive</th>\n",
       "      <td>False Positive(Type I Error)</td>\n",
       "      <td>True Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Actual Negative  \\\n",
       "                                                   \n",
       "Predicted Negative                 True Negative   \n",
       "Predicted Positive  False Positive(Type I Error)   \n",
       "\n",
       "                                  Actual Positive  \n",
       "                                                   \n",
       "Predicted Negative  Flase Negative(Type II Error)  \n",
       "Predicted Positive                  True Positive  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionmatrix = pandas.DataFrame({\" \":[\"Predicted Negative\",\"Predicted Positive\"], \"Actual Negative\":[\"True Negative\",\"False Positive(Type I Error)\"], \"Actual Positive\":[\"Flase Negative(Type II Error)\",\"True Positive\"]})\n",
    "confusionmatrix.set_index([\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual Normal</th>\n",
       "      <th>Actual bad</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Predicted Normal</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted bad</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Actual Normal Actual bad\n",
       "                                         \n",
       "Predicted Normal                         \n",
       "Predicted bad                            "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For Our topic, our confusion matrix should be like this \n",
    "confusionmatrix = pandas.DataFrame({\" \":[\"Predicted Normal\",\"Predicted bad\"], \"Actual Normal\":[\"\",\"\"], \"Actual bad\":[\"\",\"\"]})\n",
    "confusionmatrix.set_index([\" \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "A confusion matrix gives a clear overview of a classifiers performance. However, it does not provide the ability to directly rank similar classifiers. To do this, we must find a summary statistic which best fits the requirements of an intrusion detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sensitivity\n",
    "\n",
    "In our context, the sensitivity of a classifier measures the proportion of \"bad\" connections that were correctly predicted, i.e:\n",
    "$$\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}$$\n",
    "\n",
    "A function to compute the sensitivity from a confuion matrix is included below:\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sensitivity(confusion_matrix):\n",
    "    true_positives = confusion_matrix[1][1]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    return true_positives/(true_positives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity of an intrusion detection system is an important metric; it tells us the proportion of malicious traffic that we are able to detect. A model with low sensitivity will let many malicious actors through without detection. The sensitivity of our models should be considered as part of our final performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specificity\n",
    "\n",
    "In our context, the specificity of a classifier measures the proportion of normal traffic which has been classified as such, i.e:\n",
    "$$\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}$$\n",
    "\n",
    "A function to compute the sensitivity from a confusion matrix is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def specificity(confusion_matrix):\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    true_negatives = confusion_matrix[0][0]\n",
    "    return true_negatives/(true_negatives+false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positive Rate\n",
    "\n",
    "The false positive rate of a classifier measures the opposite of it's specificty: the proportion of normal traffic classified as malicious, i.e:\n",
    "$$\\frac{\\text{FP}}{\\text{TN}+\\text{FP}}$$\n",
    "\n",
    "A function to compute the false positive rate  from a confusion matrix is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def false_positive_rate(confusion_matrix):\n",
    "    true_negatives = confusion_matrix[0][0]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    \n",
    "    return false_positives/(true_negatives + false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "The accuracy of a classifier measures the proportion of the total number of predictions that were true, i.e:\n",
    "$$\\frac{\\text{TP}+\\text{TN}}{\\text{TP}+\\text{TN}+\\text{FP}+\\text{FN}}$$\n",
    "\n",
    "A function to compute accuracy from a confusion matrix is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    true_positives = confusion_matrix[1][1]\n",
    "    true_negatives = confusion_matrix[0][0]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    return (true_positives +true_negatives)/(true_positives+false_positives+true_negatives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not a good performance metric for our dataset since it is unbalanced, i.e $80\\%$ of connection events are classified as \"bad\". This allows the true positive (TP) term to dominate the equation, biasing the result. \n",
    "\n",
    "For example, a model which predicts all models as \"bad\" would achieve $80\\%$ accuracy. It would also have $0\\%$ sensitivity, indicating that this is not a good summary function.\n",
    "\n",
    "In the case of intrusion detection, this bias is of particular concern: malicious traffic within internal networks should be extremely rare for most organisations, and so this simple summary measure may cause us to optimise away the prediction of malicious traffic [6]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cohen's Kappa\n",
    "  - https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english)\n",
    "  - Here is an article on the kappa statistic: https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/\n",
    "    - The Kappa statistic is useful since it is a function of all elements of the confusion matrix. This has the potential to give a more balanced view of performance than using a metric like sensitivity on it's own.\n",
    "\n",
    "We've included an implementation of the Cohens' Kappa statistic in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kappa(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calculates the Kappa summary statistic [5] on the confusion matrix given. \n",
    "    \n",
    "    The confusion matrix should have the following structure:\n",
    "        [\n",
    "            [true_positives, false_positives],\n",
    "            [false_negatives, true_negatives],\n",
    "        ]\n",
    "    \"\"\"\n",
    "    true_positives = confusion_matrix[1][1]\n",
    "    true_negatives = confusion_matrix[0][0]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    \n",
    "    total = true_positives+false_positives+true_negatives+false_negatives\n",
    "    \n",
    "    observed_accuracy = (true_positives + true_negatives) / total\n",
    "    random_accuracy = (\n",
    "        (true_negatives+false_positives)*(true_negatives+false_negatives) \n",
    "        + (false_negatives+true_positives)*(false_positives+true_positives)\n",
    "    ) / (\n",
    "        total * total\n",
    "    )\n",
    "    \n",
    "    return (observed_accuracy - random_accuracy) / (1 - random_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** A receiver operating characteristic (ROC) curve plots the false positive rate on the X axis and the true positive rate (sensitivity) on the Y axis. This allows us to consider the trade-off between the sensitivity of a model and i'ts need. Unfortunately, it is hard for us to choose in this setting because it does not produce a single value with which models can be compared.\n",
    "\n",
    "In the case of this report, we are required to select (or develop) a single summary statistic. In the case of intrusion detection, there is a fundamental friction between sensitivity and specificity. We must be sure to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compliance Budget\n",
    "\n",
    "Within the study of security usability and economics, a compliance budget considers the impact of security policies on end-users [7]. Each end-user is thought to develop a mental model of a policies perceived benefits, against the effort required to follow it [9]. The compliance budget is the amount of effort each user is able to put into complying with a security policy, before they attempt to bypass it.\n",
    "\n",
    "In the context of intrusion detection systems (IDS), we can think of the compliance budget as the number of false alarms a security team is able to deal with in a given time period. If any more alarms are sounded in this time period, they will not be able to adequately handle all security incidents. Considering the combination of a security team and the IDS as a whole, this increase in the number of false positives in the IDS will increase the number of false negatives given by the system as a whole.\n",
    "\n",
    "Within this report, we estimate the total number of false alarms a security team would be able to handle each day. This is then used to calculate a worst-case false-positive rate for our models. This cut-off can then combined with a summary statistic to rank each mode.\n",
    "\n",
    "This number will be unique to each organisation, and dependant on a number of factors which we are unable to predict. For example, a large organisation may have a far larger security team than that of a small start-up. The KDD 99 dataset is said to simulate a \"typical U.S. Air Force LAN\" [2], and so we have assumed that they will have a well resourced security team who are able to adaquately deal with 500 false alarms per day. It has been assumed that true intrusions are extremely rare, so do not have an effect on the security teams compliance budget.\n",
    "\n",
    "In the code below, we calculate the maximum false-positive rate for each of our models. It assumes that the number connections per day is independant of the day of the week. This is a potential cause of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Maximum False Positive Rate is 0.64%\n"
     ]
    }
   ],
   "source": [
    "number_of_weeks = 9  # sourced from the KDD 99 Task Description [2]\n",
    "number_of_days = 9*7\n",
    "\n",
    "total_number_of_connections = len(connection_events)\n",
    "\n",
    "# We are using the 10 percent data set, so multiply the number of connection events by 10:\n",
    "total_number_of_connections = total_number_of_connections * 10\n",
    "\n",
    "average_connections_per_day = total_number_of_connections / number_of_days\n",
    "maximum_false_positives_per_day = 500\n",
    "maximum_false_positive_rate = maximum_false_positives_per_day/average_connections_per_day\n",
    "\n",
    "print(\"The Maximum False Positive Rate is {:.2f}%\".format(maximum_false_positive_rate*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "There are a number of approaches we could have taken when developing our models. The need to use a single performance metric to compare three models limited the potential variation in approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "#### Intro\n",
    "Logistic Regression is an appropriate regression to model when dependent variable is binary value.(i.e. in our topic, normal vs bad). In logistic regression analysis, the regression is used to describe the relationship between dependent binary variable and other independent variables.[8] \nIn addition, logistic regression is particularly appropriate for IDS, since classification of new data points is fast compared to other methods [6].\n",
    "\n",
    "The function of model can be written as:\n",
    "$logit(\\pi) = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k$\n",
    "\n",
    "Where $\\pi$ is the probability of binaray value equal to 1.\n",
    "     \n",
    "$      logit(\\pi) = log(\\pi/(1-\\pi))$\n",
    "     \n",
    "$      \\beta_0...\\beta_k$ are parameters of models.\n",
    "     \n",
    "$      x_1...x_k$ are independent variables.\n",
    "\n",
    "#### Prediction\n",
    "After we have our model, we need to use test data to see how our model is. The prediction will not be 0 or 1, it should be some probablities that \"bad\" occured. So, we need to assume if fitted value is larger than 0.5, we set it as 1, otherwise, it is 0. \n",
    "\n",
    "\n",
    "#### Model Selection\n",
    "For model selection, basically we have to methods. First, we can directly compare two models by confusion matrix if there are big differences in fitted value. Second, we can use likelihood ratio test to compare two models.\n",
    "\n",
    "Likelihood ratio test: \n",
    "The likelihood ratio test is used to test the null hypothesis that any subset of the β's is equal to 0.[14] It means if we want to use LRT for comparing our model, one model should be nested model by another. The number of β in full model is p, and yhe number of β in full model is r. The likelihood ratio test statistic is given by:\n",
    "\n",
    "\n",
    "$T = −2(l(\\hat{β}')−l(\\hat{β}))$, Where $l(\\hat{β}')$ is log likelihood function of reduced model, $l(\\hat{β})$ is log likelihood function of full model. \n",
    "\n",
    "This test statistic has a chi-square distribution with p−r degrees of freedom.\n",
    "\n",
    "#### K-fold Cross Validation\n",
    "After model selection, i get one model. At this time, I am using k-fold cross validation to reduce overfitting to make my model better. \n",
    "\n",
    "#### Implementation\n",
    "Implementation is contained in /project/shanglin-zou-model.rmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "kddata<-read.csv(\"../data/kddcup.data_10_percent.gz\")\n",
    "\n",
    "kddnames=read.table(\n",
    "    \"../data/kddcup.names\",\n",
    "    sep=\":\",\n",
    "    skip=1,\n",
    "    as.is=T\n",
    ")\n",
    "\n",
    "colnames(kddata)=c(kddnames[,1], \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#set normal col to two levels:normal=0,bad=1\n",
    "levels(kddata$normal)[which(levels(kddata$normal) !='normal.')] <- \"bad.\"\n",
    "kddata$normal<-as.numeric(factor(kddata$normal,levels = c(\"normal.\",\"bad.\"))) -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#set service col to three levels:private,http,others\n",
    "levels(kddata$service)[which(levels(kddata$service) != \"http\" & levels(kddata$service) != \"private\")] <- \"others\"\n",
    "kddata$service<-as.numeric(factor(kddata$service,levels = c(\"private\",\"http\",\"others\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#split data into train and test\n",
    "#first we split data into \"normal\" and \"bad\"\n",
    "kddata_normal<-kddata[which(kddata$normal==0),]\n",
    "kddata_bad<-kddata[which(kddata$normal==1),]\n",
    "# define an 80%/20% train/test split of the dataset\n",
    "# we sample \"normal\" and \"bad\" dataset\n",
    "sample_normal<-sample(seq_len(nrow(kddata_normal)),size=floor(.80*nrow(kddata_normal)))\n",
    "sample_bad<-sample(seq_len(nrow(kddata_bad)),size=floor(.80*nrow(kddata_bad)))\n",
    "# we get train and test for \"normal\" dataset\n",
    "train_normal<-kddata_normal[sample_normal,]\n",
    "test_normal<-kddata_normal[-sample_normal,]\n",
    "# we get train and test for \"bad\" dataset\n",
    "train_bad<-kddata_bad[sample_bad,]\n",
    "test_bad<-kddata_bad[-sample_bad,]\n",
    "#combine \"normal\" and \"bad\" train dataset to get train dataset, same for \"bad\"\n",
    "train<-rbind(train_normal,train_bad)\n",
    "test<-rbind(test_normal,test_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Selction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    " \n",
    "#model 1: variable:service(network service on the destination), logged_in(1 if successfully logged in; 0 otherwise), \n",
    "#                  srv_count(\tnumber of connections to the same service as the current connection in the past two seconds),\n",
    "#                  count(number of connections to the same host as the current connection in the past two seconds )\n",
    "#                   \n",
    "model1<-glm(normal~service+logged_in+srv_count+count, family=binomial(link=logit), data=train)\n",
    "summary(model1)\n",
    "fitted_results <- predict(model1,newdata=test,type = 'response')\n",
    "fitted_results <- ifelse(fitted_results > 0.5,1,0)\n",
    "\n",
    "confusionMatrix(factor(fitted_results),factor(test$normal),positive=\"1\",dnn = c(\"prediction\",\"actual\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#model 2: variable:service(network service on the destination), logged_in(1 if successfully logged in; 0 otherwise), \n",
    "#                  srv_count(\tnumber of connections to the same service as the current connection in the past two seconds)\n",
    "#                   \n",
    "model2<-glm(normal~service+logged_in+srv_count, family=binomial(link=logit), data=train)\n",
    "summary(model2)\n",
    "fitted_results2 <- predict(model2,newdata=test,type = 'response')\n",
    "fitted_results2 <- ifelse(fitted_results2 > 0.5,1,0)\n",
    "\n",
    "confusionMatrix(factor(fitted_results2),factor(test$normal),positive=\"1\",dnn = c(\"prediction\",\"actual\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on confusion matrix of Model 1 and Model 2, we can clearly see that the sensitivity and specificity of model 1 is larger than model 2, so we choose model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#model 3: variable:service(network service on the destination), logged_in(1 if successfully logged in; 0 otherwise), \n",
    "#                  count(number of connections to the same host as the current connection in the past two seconds )\n",
    "# \n",
    "model3<-glm(normal~service+logged_in+count, family=binomial(link=logit), data=train)\n",
    "summary(model3)\n",
    "fitted_results3 <- predict(model3,newdata=test,type = 'response')\n",
    "fitted_results3 <- ifelse(fitted_results3 > 0.5,1,0)\n",
    "\n",
    "confusionMatrix(factor(fitted_results3),factor(test$normal),positive=\"1\",dnn = c(\"prediction\",\"actual\"))\n",
    "\n",
    "anova(model3, model1,test=\"LRT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on confusion matrix of model 1 and model 3, we cannot easily pick which model is better, since sensitivity of model 1 is smaller than model 3, but specificity of model 1 is larger. Therefore, here we choose to use likelihood ratio test to determine which model is better. we assume that the parameter of srv_count = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#likelihood ratio test\n",
    "anova(model3, model1,test=\"LRT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have p-value is very small. So, we reject the hypothesis that parameter of srv_count = 0. We choose model 1 as our model. \n",
    "\n",
    "Futhermore, we use k-fold cross validation to fit our model 1. we set 10 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#k-fold cv\n",
    "#Create 10 equally size folds\n",
    "folds = createFolds(kddata$normal, k = 10)\n",
    "cv_logit = lapply(folds, function(x) {\n",
    "  training_fold = kddata[x, ]\n",
    "  test_fold = kddata[-x, ]\n",
    "  model = glm(normal~service+logged_in+srv_count+count, family=binomial(link=logit), data=training_fold)\n",
    "  y_pred = predict(model, newdata = test_fold)\n",
    "  y_pred <- ifelse(y_pred > 0.5,1,0)\n",
    "  cm<-confusionMatrix(factor(y_pred),factor(test_fold$normal),positive=\"1\",dnn = c(\"prediction\",\"actual\"))\n",
    "  return(cm$table)\n",
    "})\n",
    "cv_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Penalisation (TODO: Dan)\n",
    "\n",
    "Here we consider potential enhancements to the standard logistic regression methodology applied above. In particular, we investigate the use of Principle Component Analysis (PCA) to rank and select features of the model. Further, we consider the applicability of adding a penalization term to the maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the above mentioned techniques, the data must be cleaned and converted to a format compatible with a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "\n",
    "Selecting a subset of the KDD 99 datasets original features presents a number of advantages. When running intrusion detection systems within the internal network of an organisatoin, the system should be running and identifying threats in real-time. To do this, it needs to record, store and process the organisations traffic in a short time frame. Removing features known to be noisy and irrelevant should allow a higher percentage of network activity to be recorded, reduce the storage cost, and speed up the classification algorithm [11]. \n",
    "\n",
    "**TODO: Dan** Write about theory of PCA.\n",
    "\n",
    "In this section we attempt to use Principle Component Analysis (PCA) to identify important features, and reduce the dimensionality of the dataset. The code below uses scikit-learn's [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) component to find the principle components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the principle components have been computed, it is important to consider:\n",
    "  1. The individual impact of each principle component on the dataset.\n",
    "  2. The impact on each feature of the KDD-99 on the principle components.\n",
    "  \n",
    "To do this we create the following two plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first plot shows that the variance is equally distributed across each of the principle components. In fact, to explain 95% of the original datasets variance, 83 principle components are required. The second plot shows the relative importance of each feature in the KDD-99 dataset as determined by PCA. It similarly shows that the importance of each feature is relatively well distributed.\n",
    "\n",
    "From this analysis, the dimensionality reduction available from projecting the data into the PCA-space should be minimal. Further, we can determine that the best results will be found by using all features in the original KDD-99 dataset. Unfortunately, this means that whilst removing features will increase the models efficiency, it will decrease the models accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Penalization\n",
    "\n",
    "Plan:\n",
    "  - Describe maximum likelihood estimation\n",
    "  - potential issues with MLE\n",
    "  - \n",
    "  \n",
    "  \n",
    "Logisitic Regression requires \n",
    "\n",
    "Penalised Regression notes [12]\n",
    "\n",
    "Problems with MLE [12]:\n",
    "\n",
    "-   Large variability: when p is large with respect to n, or when columns of X are highly correlated, the variance of β is large\n",
    "-   Lack of interpretability: if p is large, we often desire a smaller set of predictors in order to gain insight into the most relevant relationships between y and X\n",
    "\n",
    "Possible solution: use a subset of explaining variables (this is being explored elsewhere in the project).\n",
    "\n",
    "\n",
    "##### Penalised Maximum Likelihood\n",
    "\n",
    "In maximum likelihood estimation we try to maximise $\\ell(\\theta|\\boldsymbol{x})$ (or maybe the log-likelihood). In this case, we should maximise $M(\\theta) = \\ell(\\theta|\\boldsymbol{x}) - \\lambda P(\\theta)$ where:\n",
    " -   $P$ is a function that penalises \"less realistic values\" of parameters\n",
    " -   $\\lambda$ controls the trade-off (the *regularisation parameter*)\n",
    "\n",
    "Call $M(\\theta)$ the **objective function**.\n",
    "\n",
    "##### How do we construct P?\n",
    "\n",
    "Construct a function that penalises unrealistic parameter values. \n",
    "\n",
    "*What is \"less realistic\"?* \"Coefficient values around zero are more believable than those far away\" [12]\n",
    "\n",
    "Two potential penalties:\n",
    " \n",
    "  - *Lasso (L1):* \n",
    "  $P(\\vec{\\theta}) = \\sum_{j=1}^{p}{|\\theta_j - 0|}$ [13]\n",
    "\n",
    "  - *Ridge (L2):* \n",
    "  $P(\\vec{\\theta}) = \\sum_{j=1}^{p}{(\\theta_j - 0)^2}$ [13]\n",
    "\n",
    "\n",
    "Both of these metrics measure the distance from the origin, requiring the dataset to be mean-normalised.\n",
    "\n",
    "##### Considerations for the regularisation parameter ($\\lambda$)\n",
    "\n",
    "-   If $\\lambda$ is too small, we can overfit giving too high a variance.\n",
    "-   If $\\lambda$ is too large, we risk underfitting with a potential for bias. [12]\n",
    "\n",
    "From data science book [6] page 196:\n",
    "\n",
    "> Penalised regression is similar in spirit to AIC. Instead of explicitly searching through a discrete set of models, the model-fitting equation incorporates a constraint that penalises the model for too many variables (parameters). Rather than eliminating predictor variables entirely—as with stepwise, forward, and backward selection — penalised regression applies the penalty by reducing coefficients, in some cases to near zero. Common penalised regression methods are ridge regression and lasso regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses sklearns [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) implementation with ridge penalisation, using every feature as a predictor variable. It trains, then records it's predictions on the testing data for each of the k-fold train/test splits. The predictions are recorded in `testing_predictions` for later analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summary_confusion_matrix(confusion_matrices, model_name):\n",
    "    confusion_matrix = pandas.DataFrame(\n",
    "        data=sum(confusion_matrices), \n",
    "        index=['Bad', 'Normal'], \n",
    "        columns=['Bad', 'Normal'],\n",
    "    )\n",
    "\n",
    "    confusion_figure, confusion_axes = matplotlib.pyplot.subplots()\n",
    "    \n",
    "    confusion_figure.set_size_inches(10, 8)\n",
    "    confusion_axes.set_title(\n",
    "        'Confusion matrix showing the predicted vs. true class of \"normal\" \\n'\n",
    "        'and \"bad\" network connections ({}).'.format(model_name)\n",
    "    )\n",
    "    \n",
    "    seaborn.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=seaborn.color_palette(\"Blues\"),\n",
    "        vmin=0,\n",
    "        ax=summary_confusion_axes,\n",
    "    )\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE (TODO: Kish) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "kddata = read.csv(\"../data/kddcup.data_10_percent.gz\")\n",
    "kddnames = read.table(\"../data/kddcup.names\",sep = \":\", skip = 1, as.is = T)\n",
    "colnames(kddata)=c(kddnames[,1],\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "unique_values = apply(kddata, 2, unique)\n",
    "colcounts = lapply(unique_values,length)\n",
    "constants = colcounts==1\n",
    "not_constants = !constants\n",
    "pruned_kddata = kddata[,not_constants]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "levels(pruned_kddata$normal) = c(levels(pruned_kddata$normal), 'bad.')\n",
    "pruned_kddata[which(pruned_kddata$normal!='normal.'), 'normal'] = 'bad.'\n",
    "pruned_kddata[which(pruned_kddata$normal!='normal.'),]\n",
    "pruned_kddata$normal = factor(pruned_kddata$normal, levels = c('normal.','bad.'))\n",
    "\n",
    "summary(pruned_kddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dataset1 = pruned_kddata[pruned_kddata$normal==\"normal.\",]\n",
    "dataset2 = pruned_kddata[pruned_kddata$normal==\"bad.\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "sample1_indexes = sample(nrow(dataset1), size = floor(0.9*nrow(dataset1)), prob=NULL)\n",
    "sample1 = dataset1[sample1_indexes,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "sample2_indexes = sample(nrow(dataset2), size = floor(0.9*nrow(dataset2)),prob=NULL)\n",
    "sample2 = dataset2[sample2_indexes,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "main_sample1 = rbind(sample1,sample2)\n",
    "test_data1 = dataset1[-sample1_indexes,]\n",
    "test_data2 = dataset2[-sample2_indexes,]\n",
    "main_test_data = rbind(test_data1,test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "classifier = svm(formula = normal ~ ., data = main_sample1, type = \"C-classification\", kernel = \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "y_pred = predict(classifier, newdata = main_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1] 0.998158\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "c_trial = table(main_test_data$normal, y_pred)\n",
    "trial_acc = (c_trial[1,1] + c_trial[2,2])/(c_trial[1,1] + c_trial[1,2] + c_trial[2,1] + c_trial[2,2])\n",
    "trial_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.41016600e+06, 1.28800000e+02, 4.60725000e+06, 2.46100000e+02,\n",
       "        3.43872300e+06, 1.83700000e+02],\n",
       "       [6.09023170e+07, 4.64700000e+02, 4.22044035e+08, 3.22000000e+03,\n",
       "        4.43691940e+08, 3.38520000e+03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%R rm(classifier, y_pred); gc()  # Cleanup and free RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "folds = createFolds(main_sample1$normal, k = 10)\n",
    "\n",
    "confusion_matrices = lapply(folds, function(x) {\n",
    "  training_fold = main_sample1[-x, ]\n",
    "  test_fold = main_sample1[x, ]\n",
    "  classifier = svm(formula = normal ~ ., data = training_fold, type = \"C-classification\", kernel = \"linear\")\n",
    "  y_pred = predict(classifier, newdata = test_fold)\n",
    "  cm = table(test_fold$normal, y_pred)\n",
    "  return(cm)\n",
    "})\n",
    "\n",
    "summary_matrix = Reduce('+', confusion_matrices)\n",
    "summary_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "accuracies = lapply(confusion_matrices, function(cm) {\n",
    "  accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])\n",
    "  return(accuracy)\n",
    "})\n",
    "\n",
    "Macc = mean(as.numeric(accuracies))\n",
    "Macc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "### Plan\n",
    "\n",
    "Here we apply performance metric to models.\n",
    "\n",
    "Create a summary table with:\n",
    "  - model\n",
    "  - summary statistic \n",
    "    - maybe multiple?\n",
    "    \n",
    "Talk about how some models performed better under different conditions.\n",
    "\n",
    "How applicable is each model for use in live intrusion detection in e.g. a corporate internal network.\n",
    "\n",
    "\n",
    "### Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Performance Metric\n",
    "\n",
    "### Plan\n",
    "Here we evaluate how good our performance metric was.\n",
    "\n",
    "Key points:\n",
    "  - Limitations of our performance metric:\n",
    "    - In the end, in order to pick a \"winner\" we need to decide on a single summary statistic. This approach to picking a model then becomes brute force and has loses the subtle differences in the models.\n",
    "    - The class ratios in our dataset do not match real-world ratios, this means our performance metric may overly value certain criteria over others. (TODO: this needs to be more concrete).\n",
    "    - Our performance metric in unable to take into account the conditions that an intrusion detection system would need to run under. Bla bla bla theres always a trade-off between model performance and speed.\n",
    "\n",
    "### Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]: rpy2, https://rpy2.bitbucket.io/.\n",
    "\n",
    "[2]: KDD-CUP-99 Task Description, http://kdd.ics.uci.edu/databases/kddcup99/task.html.\n",
    "\n",
    "[3]: Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.\n",
    "\n",
    "[4]: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "[5]: Confusion Matrix – Another Single Value Metric – Kappa Statistic, https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/\n",
    "\n",
    "[6]: Practical Statistics for Data Science, 1st ed., by Peter Bruce and Andrew Bruce (O’Reilly Media, 2017).\n",
    "\n",
    "[7]: Beautement, Adam, M. Angela Sasse, and Mike Wonham. \"The compliance budget: managing security behaviour in organisations.\" Proceedings of the 2008 New Security Paradigms Workshop. ACM, 2009.\n",
    "\n",
    "[8]: What-is-logistic-regression: https://www.statisticssolutions.com/what-is-logistic-regression/\n",
    "\n",
    "[9]: Weirich, Dirk. “Persuasive password security.” CHI Extended Abstracts (2001).\n",
    "\n",
    "[10]: Principal Component Analysis (PCA) for Feature Selection and some of its Pitfalls, http://jotterbach.github.io/2016/03/24/Principal_Component_Analysis/\n",
    "\n",
    "[11]: Liu, Huan, and Lei Yu. \"Toward integrating feature selection algorithms for classification and clustering.\" IEEE Transactions on knowledge and data engineering 17.4 (2005): 491-502. https://doi.org/10.1109/TKDE.2005.66\n",
    "\n",
    "[12]: BST 764: Applied Statistical Modelling, https://web.as.uky.edu/statistics/users/pbreheny/764-F11/notes/8-30.pdf\n",
    "\n",
    "[13]: Penalized logistic regression with rare events: Preliminary Results, p12 http://prema.mf.uni-lj.si/files/PREMAPreliminary_Lara_AS2015_f98.pdf.\n",
    "\n",
    "[14]: Logistic-Regression: Likelihood Ratio (or Deviance) Test, https://onlinecourses.science.psu.edu/stat501/node/374/ \n"
    "[15]: Katos, V., 2007. Network intrusion detection: Evaluating cluster, discriminant, and logit analysis. Information Sciences, 177(15), pp.3060-3073. https://doi.org/10.1016/j.ins.2007.02.034 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
