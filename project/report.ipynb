{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrusion Detection on the KDD Cup 99 Data Set\n",
    "*An investigation into models and performance metrics for the classification of network data.*\n",
    "\n",
    "--- \n",
    "\n",
    "**TODO** Standardize on what we consider a positive and negative result e.g. is normal positive, or is bad positive? This will ensure our write-ups and code are consistent and make sense together\n",
    "  - Positive :: detection of intrusion (`bad.` label)\n",
    "  - Negative :: detected as normal traffic (`normal.` label)\n",
    "\n",
    "**TODO** Use the same labels for normal and bad data. We've decided on `normal.` and `bad.`.\n",
    "\n",
    "**TODO** Introduction. Should include:\n",
    "\n",
    "  * Introduce the KDD Cup:\n",
    "    - what was it's aims?\n",
    "    - what does the data set look like? features etc.\n",
    "  * Prior work on the kdd 99 data set\n",
    "    - extensively studied\n",
    "    - papers, code we found, etc.\n",
    "  * Introduce our intended approach, talk about:\n",
    "    - choose a number of approaches to compare\n",
    "    - lots of thought has gone into our performance metric <-- make this clear\n",
    "    \n",
    "We decided to consider three approaches to this problem - logistic regression, logistic regression with penalisation, and support vector machines. We have allocated the work as follows:\n",
    "  1. Shanglin will work on the logistic regression model.\n",
    "  2. Daniel will work on an extension of logistic regression using penalisation and feature selection.\n",
    "  3. Kishalay will work on a model using support vector machines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook needs to be setup with the required libraries and dependencies loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from statsmodels import api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For inline plots within the notebook\n",
    "%matplotlib inline\n",
    "# Allows code cells to be intrepreted as R (put %%R on the first line) [1]\n",
    "%load_ext rpy2.ipython\n",
    "# Render R output as HTML\n",
    "from functools import partial\n",
    "from rpy2.ipython import html\n",
    "html.html_rdataframe=partial(html.html_rdataframe, table_class=\"docutils\")\n",
    "html.init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(caret)\n",
    "library(ggplot2)\n",
    "library(data.table)\n",
    "library(lattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed the R and Python random number generators to ensure we get consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = numpy.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source (TODO: Dan)\n",
    "\n",
    "The dataset we used for this project was originally used in the 1999 KDD Intrusion Detection Contest [2] [3]. This data\n",
    "\n",
    "Each row in the data represents a single TCP connection, as described in the original task description [2]:\n",
    "> A connection is a sequence of TCP packets starting and ending at some well defined times, between which data flows to and from a source IP address to a target IP address under some well defined protocol.  Each connection is labeled as either normal, or as an attack, with exactly one specific attack type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `connection_label` column describes the source of each connection; either the name of the red-team which caused the event, or the string `normal.` which indicates normal network behaviour. \n",
    "\n",
    "The task is to create a model which can separate red-team behavour from normal network behaviour, so we need to group the data into two labels: `normal` and `bad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'connection_label']\n",
    "connection_events = pandas.read_csv('http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz', names=columns)  # [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_events.head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `connection_label` column describes the source of each connection; either the name of the red-team which caused the event, or the string `normal.` which indicates normal network behaviour. \n",
    "\n",
    "The task is to create a model which can separate red-team behavour from normal network behaviour, so group the data into two labels: `normal` and `bad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(connection_label):\n",
    "    return 'normal' if connection_label == 'normal.' else 'bad'\n",
    "\n",
    "connection_events['connection_label'] = connection_events['connection_label'].apply(func=generate_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, separate out the labels from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_labels = connection_events.filter(['connection_label'], axis='columns')\n",
    "connection_events = connection_events.drop(['connection_label'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics for Classification\n",
    "From spec:\n",
    "> - Together agree and test a performance metric.\n",
    ">   - Half of the effort should be devoted to exploring appropriate performance measures.\n",
    ">   - You should create a test and validation dataset, but you may choose how to do this.\n",
    "\n",
    "\n",
    "From spec:\n",
    "> * Think about the circumstances by which your chosen performance metric\n",
    "will lead to real-world generalisability, and how it might compromise this for\n",
    "the purpose of standardization.\n",
    "> * Demonstrate this with data and/or simulation;\n",
    "for example, if you believe that you can predict new types of data, you could\n",
    "demonstrate this by leaving out some types of data and observing your perfor-\n",
    "mance. \n",
    "> * Examine in what sense your groupâ€™s best method is truly best.\n",
    "\n",
    "### Cross-Validation (TODO: Kish)\n",
    "  - Decided on k-fold, why? Brief comparison to other types.\n",
    "  \n",
    "\n",
    "The code below performs 10-fold cross-validation, providing the array `train_test_splits`. It does this using the [model selection](http://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold) part of the scikit-learn library [5].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_splitter = model_selection.StratifiedKFold(n_splits=10,  random_state=random_state)\n",
    "train_test_splits = k_fold_splitter.split(\n",
    "    connection_events,  # data to be split\n",
    "    connection_labels,  # target/class to split by\n",
    ")\n",
    "\n",
    "# Force evaluation of the train_test_splits generator into a list. This needs to be\n",
    "# done before it's sent to R.\n",
    "train_test_splits = [\n",
    "    [training_indices, testing_indices]\n",
    "    for training_indices, testing_indices in train_test_splits\n",
    "]\n",
    "\n",
    "# Here train_test_splits is a list, where each item represents a single 90/10 split of \n",
    "# training and testing data respectively. The values contained are 0-based indices\n",
    "# of the rows in each part of the split, i.e:\n",
    "#\n",
    "#   train_test_splits = [\n",
    "#      (indices_of_training_samples, indices_of_test_samples),  # first split\n",
    "#      (indices_of_training_samples, indices_of_test_samples),  # second split\n",
    "#      ...\n",
    "#      (indices_of_training_samples, indices_of_test_samples),  # kth split\n",
    "#  ]\n",
    "#\n",
    "# These indices can then be used to fetch the data samples and their labels,\n",
    "# ready for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports the training and testing sets into the R session, ready for analysis and modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%Rpush train_test_splits connection_events connection_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The helper functions below extract training and test rows from a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "get_training_rows <- function(dataframe, train_test_split) {\n",
    "    indices <- train_test_split[[1]]  # Training indices are the first item in a train_test_split\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1. Correct for this by incrementing each index by 1:\n",
    "    indices <- indices + 1\n",
    "    \n",
    "    dataframe[indices,]\n",
    "}\n",
    "\n",
    "get_testing_rows <- function(dataframe, train_test_split) {\n",
    "    indices <- train_test_split[[2]]  # Testing indexes are the second item in a train_test_split\n",
    "    \n",
    "    # Python indices start at 0, whilst R indices start at 1. Correct for this by incrementing each index by 1:\n",
    "    indices <- indices + 1  \n",
    "    \n",
    "    dataframe[indices,]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (TODO: Shanglin)\n",
    "- We chose to use one, why?\n",
    "- What other options were there\n",
    "\n",
    "\n",
    "### Summary Statistics (TODO: Shanglin, Dan, Kish)\n",
    "\n",
    "Look here for table with lots of them on: https://en.wikipedia.org/wiki/Sensitivity_and_specificity and here is a comparison of two new ones: http://standardwisdom.com/softwarejournal/2011/12/matthews-correlation-coefficient-how-well-does-it-do/\n",
    "\n",
    "  - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(confusion_matrix):\n",
    "    true_positives = confusion_matrix[0][0]\n",
    "    true_negative = confusion_matrix[1][1]\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    false_negative = confusion_matrix[1][0]\n",
    "    return (true_positives +true_negative)/(true_positives+false_positive+true_negatives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Sensitivity\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(confusion_matrix):\n",
    "    true_positives = confusion_matrix[0][0]\n",
    "    false_negatives = confusion_matrix[1][0]\n",
    "    return true_positives/(true_positives+false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Specificity\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(confusion_matrix):\n",
    "    false_positives = confusion_matrix[0][1]\n",
    "    true_negatives = confusion_matrix[1][1]\n",
    "    return true_negatives/(true_negatives+false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Kappa  (https://stats.stackexchange.com/questions/82162/cohens-kappa-in-plain-english). Here is an article on the kappa statistic: https://standardwisdom.com/softwarejournal/2011/12/confusion-matrix-another-single-value-metric-kappa-statistic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa(confusion_matrix):\n",
    "    #                 Cats Dogs  (truths)\n",
    "    #     Cats    | 22 | 9  |\n",
    "    #     Dogs   | 7  | 13 |\n",
    "    #     (predictions)\n",
    "\n",
    "    #     Ground truth: Cats (29), Dogs (22)\n",
    "    #     Machine Learning Classifier: Cats (31), Dogs (20)\n",
    "    #     Total: (51)\n",
    "    #     Observed Accuracy: ((22 + 13) / 51) = 0.69\n",
    "    #     Expected Accuracy: ((29 * 31 / 51) + (22 * 20 / 51)) / 51 = 0.51\n",
    "    #     Kappa: (0.69 - 0.51) / (1 - 0.51) = 0.37\n",
    "    #\n",
    "    # TODO: Dan\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Models\n",
    "\n",
    "**TODO** Dan\n",
    "\n",
    "From spec:\n",
    "> For example, you could look to predict the next event on each\n",
    "edge based on past events on this edge, or you could model the network at a\n",
    "more global level, and many other approaches are possible.\n",
    "\n",
    "**TODO**: We decided to model it on a global scale, why?\n",
    "   - Real-llfe it is hard to keep track of state between events (especially true for high-traffic websites), it's easier to classify each incoming event individually as it goes with a previously trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (TODO: Shanglin)\n",
    "**TODO** k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "kddata<-read.csv(\"../data/kddcup.data_10_percent.gz\")\n",
    "\n",
    "kddnames=read.table(\n",
    "    \"../data/kddcup.names\",\n",
    "    sep=\":\",\n",
    "    skip=1,\n",
    "    as.is=T\n",
    ")\n",
    "\n",
    "colnames(kddata)=c(kddnames[,1], \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#set normal col to two levels:normal=0,non-normal=1\n",
    "levels(kddata$normal)[which(levels(kddata$normal) !='normal.')] <- \"non-normal.\"\n",
    "kddata$normal<-as.numeric(factor(kddata$normal,levels = c(\"normal.\",\"non-normal.\"))) -1 \n",
    "\n",
    "kddata_normal<-kddata[which(kddata$normal==0),]\n",
    "kddata_nonnormal<-kddata[which(kddata$normal==1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#set service col to three levels:private,http,others\n",
    "levels(kddata$service)[which(levels(kddata$service) != \"http\" & levels(kddata$service) != \"private\")] <- \"others\"\n",
    "kddata$service<-as.numeric(factor(kddata$service,levels = c(\"private\",\"http\",\"others\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#train and test\n",
    "kddata_normal<-kddata[which(kddata$normal==0),]\n",
    "kddata_nonnormal<-kddata[which(kddata$normal==1),]\n",
    "\n",
    "sample_normal<-sample(seq_len(nrow(kddata_normal)),size=floor(.80*nrow(kddata_normal)))\n",
    "sample_nonnormal<-sample(seq_len(nrow(kddata_nonnormal)),size=floor(.80*nrow(kddata_nonnormal)))\n",
    "\n",
    "train_normal<-kddata_normal[sample_normal,]\n",
    "test_normal<-kddata_normal[-sample_normal,]\n",
    "\n",
    "train_nonnormal<-kddata_nonnormal[sample_nonnormal,]\n",
    "test_nonnormal<-kddata_nonnormal[-sample_nonnormal,]\n",
    "\n",
    "train<-rbind(train_normal,train_nonnormal)\n",
    "test<-rbind(test_normal,test_nonnormal)\n",
    "\n",
    "head(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "#fit in logit model\n",
    "model1<-glm(normal~service+logged_in+srv_count, family=binomial(link='logit'), data=train)\n",
    "\n",
    "summary(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "fitted.results <- predict(model1,newdata=test,type = 'response')\n",
    "fitted.results <- ifelse(fitted.results > 0.5,1,0)\n",
    "confusion<-table(fitted.results,test$normal)\n",
    "accuracy<-(confusion[1,1]+confusion[2,2])/length(fitted.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Penalisation (TODO: Dan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUPPORT VECTOR MACHINE (TODO: Kish) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(data.table)\n",
    "kddata = read.csv(\"../data/kddcup.data_10_percent.gz\")\n",
    "kddnames = read.table(\"../data/kddcup.names\",sep = \":\", skip = 1, as.is = T)\n",
    "colnames(kddata)=c(kddnames[,1],\"normal\")\n",
    "kddata\n",
    "length(kddata$normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "colcounts = lapply(unique_temp,length)\n",
    "constants = colcounts==1\n",
    "constants\n",
    "not_constants = !constants\n",
    "not_constants\n",
    "pruned_kddata = kddata[,not_constants]\n",
    "pruned_kddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "levels(pruned_kddata$normal) <- c(levels(pruned_kddata$normal), 'bad.')\n",
    "pruned_kddata[which(pruned_kddata$normal!='normal.'), 'normal'] <- 'bad.'\n",
    "pruned_kddata[which(pruned_kddata$normal!='normal.'),]\n",
    "pruned_kddata$normal = factor(pruned_kddata$normal, levels = c('normal.','bad.'))\n",
    "pruned_kddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "dataset1 = pruned_kddata[pruned_kddata$normal==\"normal.\",]\n",
    "dataset1\n",
    "dataset2 = pruned_kddata[pruned_kddata$normal==\"bad.\",]\n",
    "dataset2\n",
    "length(dataset2$normal)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "sample1_indexes = sample(nrow(dataset1), size = floor(0.8*nrow(dataset1)), prob=NULL)\n",
    "sample1 = dataset1[sample1_indexes,]\n",
    "sample1\n",
    "length((sample1$normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "sample2_indexes = sample(nrow(dataset2), size = floor(0.2*nrow(dataset2)),prob=NULL)\n",
    "sample2 = dataset2[sample2_indexes,]\n",
    "sample2\n",
    "length((sample2$normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "main_sample1 = rbind(sample1,sample2)\n",
    "main_sample1\n",
    "test_data1 = dataset1[-sample1_indexes,]\n",
    "test_data1\n",
    "length(test_data1$normal)\n",
    "test_data2 = dataset2[-sample2_indexes,]\n",
    "length(test_data2$normal)\n",
    "main_test_data = rbind(test_data1,test_data2)\n",
    "main_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages('e1071')\n",
    "library(e1071)\n",
    "classifier = svm(formula = normal ~ ., data = main_sample1, type = \"C-classification\", kernel = \"linear\")\n",
    "\n",
    "y_pred = predict(classifier, newdata = main_test_data)\n",
    "table(main_test_data$normal, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages('caret')\n",
    "library(caret)\n",
    "library(e1071)\n",
    "folds = createFolds(main_sample1$normal, k = 10)\n",
    "cv = lapply(folds, function(x) {\n",
    "  training_fold = main_sample1[-x, ]\n",
    "  test_fold = main_sample1[x, ]\n",
    "  classifier = svm(formula = normal ~ ., data = training_fold, type = \"C-classification\", kernel = \"linear\")\n",
    "  y_pred = predict(classifier, newdata = test_fold)\n",
    "  cm = table(test_fold$normal, y_pred)\n",
    "  accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])\n",
    "  return(accuracy)\n",
    "  })\n",
    "cv\n",
    "Macc = mean(as.numeric(cv))\n",
    "Macc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Here we apply performance metric to models\n",
    "\n",
    "## Evaluation of Performance Metric\n",
    "Here we evaluate how good our performance metric was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]: rpy2, https://rpy2.bitbucket.io/.\n",
    "\n",
    "[2]: KDD-CUP-99 Task Description, http://kdd.ics.uci.edu/databases/kddcup99/task.html.\n",
    "\n",
    "[3]: Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.\n",
    "\n",
    "[4]: Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
