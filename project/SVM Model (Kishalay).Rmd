---
title: "SVM Model (Kishalay)"
author: "Kishalay"
date: "23 November 2018"
output: html_document
---

```{r}
#CODE CHUNK 1

library(data.table)
kddata = read.csv("../data/kddcup.data_10_percent.gz")
kddnames = read.table("../data/kddcup.names",sep = ":", skip = 1, as.is = T)
colnames(kddata)=c(kddnames[,1],"normal")
head(kddata)
set.seed(1)

```

In Code Chunk 1, I am calling the library function data.table, which allows me to read in the data. Then, I rename the columns using the file kddnames and just check the first 6 observations of each column to ensure that my changes have taken effect. I am also setting a seed value for the random number generator.



```{r}
#CODE CHUNK 2

unique_values = apply(kddata,2,unique)
colcounts = lapply(unique_values,length)
constants = colcounts==1
not_constants = !constants
pruned_kddata = kddata[,not_constants]
head(pruned_kddata)

```

In Code Chunk 2, I am trying to find out if any of the 42 columns consist of just one value, throughout. If such a column(s), exists, I will remove that column from my dataset. This is because, since these columns have the same value for both categories of the response variable (normal, bad), it is logical to assume that they won't be affecting the response variable at all. 


```{r}
#CODE CHUNK 3

levels(pruned_kddata$normal) = c(levels(pruned_kddata$normal), 'bad.')
pruned_kddata[which(pruned_kddata$normal!='normal.'), 'normal'] = 'bad.'
head(pruned_kddata[which(pruned_kddata$normal!='normal.'),])
pruned_kddata$normal = factor(pruned_kddata$normal, levels = c('normal.','bad.'))
nrow(pruned_kddata[which(pruned_kddata$normal=='bad.'),])
```

In Code Chunk 3, I am renaming every response corresponding to malicious attacks, as 'bad.'. I have already explained my reasons for doing so in the Main Report. 


```{r}
#CODE CHUNK 4

dataset1 = pruned_kddata[pruned_kddata$normal=="normal.",]
dataset2 = pruned_kddata[pruned_kddata$normal=="bad.",]

```

In Code Chunk 4, I am dividing the new dataset into 2 parts - the first part consists of those rows whose values for the Normal column correspond to 'normal.'. The second part consists of those rows whose values for the Normal column correspond to 'bad.'.

```{r}
#CODE CHUNK 5

sample1_indexes = sample(nrow(dataset1), size = floor(0.8*nrow(dataset1)), prob=NULL)
sample1 = dataset1[sample1_indexes,]

```

In Code Chunk 5, I am sampling 90% of the observations from the first part of the dataset.

```{r}
#CODE CHUNK 6

sample2_indexes = sample(nrow(dataset2), size = floor(0.9*nrow(dataset2)),prob=NULL)
sample2 = dataset2[sample2_indexes,]

```

In Code Chunk 6, I am sampling 90% of the observations from the second part of the dataset.

```{r}
#CODE CHUNK 7

main_sample1 = rbind(sample1,sample2)
test_data1 = dataset1[-sample1_indexes,]
test_data2 = dataset2[-sample2_indexes,]
main_test_data = rbind(test_data1,test_data2)

```


In Code Chunk 7, I bound the two samples from CC6 to create the Training Set. I called it main_sample1.
Similarly, I bound the observations left over after the two sampling operations to create the Test Set. I called it main_test_data.

```{r}
#CODE CHUNK 8

install.packages('caret')
install.packages('e1071')
library(caret)
library(e1071)

```

In Code Chunk 8, I am installing some packages necessary for carrying out SVM later on.

```{r}
#CODE CHUNK 9

classifier = svm(formula = normal ~ ., data = main_sample1, type = "C-classification", kernel = "linear")
y_pred = predict(classifier, newdata = main_test_data)
c_trial = table(main_test_data$normal, y_pred)
trial_acc = (c_trial[1,1] + c_trial[2,2])/(c_trial[1,1] + c_trial[1,2] + c_trial[2,1] + c_trial[2,2])
trial_acc

```

In Code Chunk 9, I am carrying out the analysis using SVM. I am not implementing k fold cross validation yet.

```{r}
#CODE CHUNK 10

folds = createFolds(main_sample1$normal, k = 10)

confusion_matrices = lapply(folds, function(x) {
  training_fold = main_sample1[-x, ]
  test_fold = main_sample1[x, ]
  classifier = svm(formula = normal ~ ., data = training_fold, type = "C-classification", kernel = "linear")
  y_pred = predict(classifier, newdata = test_fold)
  cm = table(test_fold$normal, y_pred)
  return(cm)
})

summary_matrix = Reduce('+', confusion_matrices)
summary_matrix

```

In Code Chunk 10, I am carrying out k fold cross validation. However, I am doing it on 90% of the total data, since, when I tried this on the actual data, it ran for more than 2 hours, without a result, and finally reported a warning that the system was reaching the maximum number of iterations. I am trying to store the Confusion Matrices obtained on each iteration and finally adding them to get the main Confusion Matrix.

```{r}
#CODE CHUNK 11

accuracies = lapply(confusion_matrices, function(cm) {
  accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})

Macc = mean(as.numeric(accuracies))
Macc

```



In Code Chunk 11, I am calculating the accuracy of each iteration, and finally, obtaining their mean. This mean is theoretically, a better value since it is devoid of variablity errors. 

NOTE - Code Chunk 11 is run on 90% data since my machine crashed when I tried to run it on the entire data. Also, this code itself ran once, and crashed a couple of times, after 2 hours. 




